{"version":3,"sources":["webpack:///path---depthkit-js-4f2ef2958d45781add5b.js","webpack:///./.cache/json/depthkit-js.json"],"names":["webpackJsonp","379","module","exports","data","markdownRemark","html","frontmatter","date","components","path","about","cover","credits","title","press","links","embed","tags","excerpt","pathContext","prev","id","next"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAkBC,MAAQC,gBAAkBC,KAAA,GAAAC,aAAyBC,KAAA,iBAAAC,aAAA,sEAAAC,KAAA,eAAAC,MAAA,4LAAAC,MAAA,uBAAAC,QAAA,GAAAC,MAAA,cAAAC,SAAAC,QAAA,kLAAAC,MAAA,GAAAC,MAAA,+CAAAC,QAAA,oDAA+sBC,aAAgBC,MAAQf,KAAA,GAAAgB,GAAA,0GAAAf,aAAwIC,KAAA,4BAAAE,KAAA,WAAAI,MAAA,UAAAE,QAAA,+IAAAH,QAAA,2FAAAE,QAAA,oXAAAN,aAAA,2FAAAG,MAAA,WAAAD,MAAA,2fAAAM,MAAA,oMAA++CE,QAAA,6DAAAD,MAAA,uDAAmLK,MAASjB,KAAA,GAAAgB,GAAA,+GAAAf,aAA6IC,KAAA,4BAAAE,KAAA,aAAAI,MAAA,YAAAE,QAAA,wFAAAH,QAAA,2IAAAE,QAAA,kjBAAAN,aAAA,sEAAAG,MAAA,gBAAAD,MAAA,ubAAAM,MAAA,oMAAslDE,QAAA,mEAAAD,MAAA","file":"path---depthkit-js-4f2ef2958d45781add5b.js","sourcesContent":["webpackJsonp([121080360342820],{\n\n/***/ 379:\n/***/ (function(module, exports) {\n\n\tmodule.exports = {\"data\":{\"markdownRemark\":{\"html\":\"\",\"frontmatter\":{\"date\":\"April 23, 2018\",\"components\":[[\"code\",\"Javascript, GLSL\"],[\"software\",\"DepthKit\"],[\"3d\",\"Three.js\"]],\"path\":\"/depthkit-js\",\"about\":\"DepthKit.js is a plugin for visualising DepthKit volumteric captures using Three.js in WebGL. The plugin requires Three.js and a DepthKit combined-per-pixel video export from Visualise.\",\"cover\":\"depthkitjs_cover.png\",\"credits\":\"\",\"title\":\"DepthKit.js\",\"press\":[],\"links\":[[\"Github\",\"https://github.com/juniorxsound/DepthKit.js\"],[\"Documentation\",\"https://juniorxsound.github.io/DepthKit.js/\"],[\"npm package\",\"https://www.npmjs.com/package/depthkit\"]],\"embed\":\"\",\"tags\":[\"Augmented Reality\",\"Virtual Reality\",\"Tools\"],\"excerpt\":\"A WebVR plugin for rendering volumetric video.\"}}},\"pathContext\":{\"prev\":{\"html\":\"\",\"id\":\"/home/travis/build/juniorxsound/portfolio/src/pages/projects/twit.md absPath of file >>> MarkdownRemark\",\"frontmatter\":{\"date\":\"2018-04-23T12:28:00+00:02\",\"path\":\"/twit-ar\",\"title\":\"Twit.AR\",\"links\":[[\"Documentation\",\"http://itp.orfleisher.com/2017/10/21/context-with-twitter-ar/\"],[\"Presskit\",\"http://orfleisher.com/twitter_ar/mediakit.zip\"]],\"credits\":\"Developed with <a target=\\\"_blank\\\" href=\\\"http://agermanidis.com\\\">Anastasis Germanidis</a>\",\"press\":[[\"prosthetic knowledge\",\"http://prostheticknowledge.tumblr.com/post/166745203731/twitar-coding-experiment-from-or-fleisher-and\"],[\"Next Reality\",\"https://mobile-ar.reality.news/news/bizarre-ar-experiment-serves-tweets-for-everything-your-iphone-can-see-0180743/\"],[\"Alphr\",\"http://www.alphr.com/twitter/1007491/twitter-in-augmented-reality-looks-like-a-living-nightmare\"]],\"components\":[[\"code\",\"Swift\"],[\"software\",\"Inception v3, CoreML, Twitter API & Swifter\"],[\"3d\",\"ARKit\"]],\"cover\":\"twit.jpg\",\"about\":\"TwitAR is a speculative satirical experiment that examines how Twitter tweets could be visualized in Augmented Reality.\\nTwitAR tries to playfully imagine what would happen if Twitter intruded our everyday reality. The experiment uses Apple’s ARKit to visualize tweets in Augmented Reality on the world itself.\\nTo match the context to what the user is seeing it uses Machine Learning (Apple’s CoreML) to classify the objects you are looking at and pulls tweets from Twitter based on this classification.\",\"embed\":\"<iframe width=\\\"100%\\\" height=\\\"450\\\" src=\\\"https://www.youtube.com/embed/LVnUHWsGEaQ?rel=0&amp;controls=0&amp;showinfo=0\\\" frameborder=\\\"0\\\" allow=\\\"autoplay; encrypted-media\\\" allowfullscreen></iframe>\",\"excerpt\":\"Predict how long people have to live in augmented reality.\",\"tags\":[\"Augmented Reality\",\"Machine Learning\",\"Experiment\"]}},\"next\":{\"html\":\"\",\"id\":\"/home/travis/build/juniorxsound/portfolio/src/pages/projects/skeletron.md absPath of file >>> MarkdownRemark\",\"frontmatter\":{\"date\":\"2018-04-23T12:23:00+00:02\",\"path\":\"/skeletron\",\"title\":\"Skeletron\",\"links\":[[\"Presskit\",\"https://drive.google.com/drive/folders/18uzf-grMetd9bZPNMHDNs7IZWZHNFmKY\"]],\"credits\":\"Developed with <a target=\\\"_blank\\\" href=\\\"https://drorayalon.com\\\">Dror Ayalon</a>. <br />Attribution: VNect ML Model VNect TensorFlow Port\",\"press\":[[\"The Next Web\",\"https://thenextweb.com/artificial-intelligence/2018/01/30/programmers-use-tensorflow-ai-to-turn-any-webcam-into-microsoft-kinect/\"],[\"Tech Radar\",\"https://www.techradar.com/news/ai-developers-can-turn-any-webcam-into-a-kinect\"],[\"GeekTime\",\"https://www.geektime.co.il/developers-create-kinect-with-tensorflow-and-webcam/\"],[\"Android Headlines\",\"https://www.androidheadlines.com/2018/01/tensorflow-unity-turn-webcams-into-ai-powered-ar-systems.html\"],[\"FossBytes\",\"https://fossbytes.com/programmers-transform-a-10-webcam-into-microsoft-kinect/\"]],\"components\":[[\"code\",\"C#, HLSL Python\"],[\"software\",\"Unity3D\"],[\"3d\",\"Tensorflow\"]],\"cover\":\"skeletron.jpg\",\"about\":\"Skeletron is a system that predicts joints and human skeleton position from real-time video taken by any RGB camera, such as a webcam. The system sends the data about the position of the human body to Unity, a 3D game development engine, to allow engineers, artists, and creative technologists to use it to develop digital experiences.\\nSkeletron was developed thanks and as a part of NYU ITP Xstory grant (Experiments in Storytelling).\",\"embed\":\"<iframe width=\\\"100%\\\" height=\\\"500\\\" src=\\\"https://www.youtube.com/embed/l_owi316cE8?rel=0&amp;controls=0&amp;showinfo=0\\\" frameborder=\\\"0\\\" allow=\\\"autoplay; encrypted-media\\\" allowfullscreen></iframe>\",\"excerpt\":\"Predict joints and human skeleton position from real-time video.\",\"tags\":[\"Machine Learning\",\"Tools\"]}}}}\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// path---depthkit-js-4f2ef2958d45781add5b.js","module.exports = {\"data\":{\"markdownRemark\":{\"html\":\"\",\"frontmatter\":{\"date\":\"April 23, 2018\",\"components\":[[\"code\",\"Javascript, GLSL\"],[\"software\",\"DepthKit\"],[\"3d\",\"Three.js\"]],\"path\":\"/depthkit-js\",\"about\":\"DepthKit.js is a plugin for visualising DepthKit volumteric captures using Three.js in WebGL. The plugin requires Three.js and a DepthKit combined-per-pixel video export from Visualise.\",\"cover\":\"depthkitjs_cover.png\",\"credits\":\"\",\"title\":\"DepthKit.js\",\"press\":[],\"links\":[[\"Github\",\"https://github.com/juniorxsound/DepthKit.js\"],[\"Documentation\",\"https://juniorxsound.github.io/DepthKit.js/\"],[\"npm package\",\"https://www.npmjs.com/package/depthkit\"]],\"embed\":\"\",\"tags\":[\"Augmented Reality\",\"Virtual Reality\",\"Tools\"],\"excerpt\":\"A WebVR plugin for rendering volumetric video.\"}}},\"pathContext\":{\"prev\":{\"html\":\"\",\"id\":\"/home/travis/build/juniorxsound/portfolio/src/pages/projects/twit.md absPath of file >>> MarkdownRemark\",\"frontmatter\":{\"date\":\"2018-04-23T12:28:00+00:02\",\"path\":\"/twit-ar\",\"title\":\"Twit.AR\",\"links\":[[\"Documentation\",\"http://itp.orfleisher.com/2017/10/21/context-with-twitter-ar/\"],[\"Presskit\",\"http://orfleisher.com/twitter_ar/mediakit.zip\"]],\"credits\":\"Developed with <a target=\\\"_blank\\\" href=\\\"http://agermanidis.com\\\">Anastasis Germanidis</a>\",\"press\":[[\"prosthetic knowledge\",\"http://prostheticknowledge.tumblr.com/post/166745203731/twitar-coding-experiment-from-or-fleisher-and\"],[\"Next Reality\",\"https://mobile-ar.reality.news/news/bizarre-ar-experiment-serves-tweets-for-everything-your-iphone-can-see-0180743/\"],[\"Alphr\",\"http://www.alphr.com/twitter/1007491/twitter-in-augmented-reality-looks-like-a-living-nightmare\"]],\"components\":[[\"code\",\"Swift\"],[\"software\",\"Inception v3, CoreML, Twitter API & Swifter\"],[\"3d\",\"ARKit\"]],\"cover\":\"twit.jpg\",\"about\":\"TwitAR is a speculative satirical experiment that examines how Twitter tweets could be visualized in Augmented Reality.\\nTwitAR tries to playfully imagine what would happen if Twitter intruded our everyday reality. The experiment uses Apple’s ARKit to visualize tweets in Augmented Reality on the world itself.\\nTo match the context to what the user is seeing it uses Machine Learning (Apple’s CoreML) to classify the objects you are looking at and pulls tweets from Twitter based on this classification.\",\"embed\":\"<iframe width=\\\"100%\\\" height=\\\"450\\\" src=\\\"https://www.youtube.com/embed/LVnUHWsGEaQ?rel=0&amp;controls=0&amp;showinfo=0\\\" frameborder=\\\"0\\\" allow=\\\"autoplay; encrypted-media\\\" allowfullscreen></iframe>\",\"excerpt\":\"Predict how long people have to live in augmented reality.\",\"tags\":[\"Augmented Reality\",\"Machine Learning\",\"Experiment\"]}},\"next\":{\"html\":\"\",\"id\":\"/home/travis/build/juniorxsound/portfolio/src/pages/projects/skeletron.md absPath of file >>> MarkdownRemark\",\"frontmatter\":{\"date\":\"2018-04-23T12:23:00+00:02\",\"path\":\"/skeletron\",\"title\":\"Skeletron\",\"links\":[[\"Presskit\",\"https://drive.google.com/drive/folders/18uzf-grMetd9bZPNMHDNs7IZWZHNFmKY\"]],\"credits\":\"Developed with <a target=\\\"_blank\\\" href=\\\"https://drorayalon.com\\\">Dror Ayalon</a>. <br />Attribution: VNect ML Model VNect TensorFlow Port\",\"press\":[[\"The Next Web\",\"https://thenextweb.com/artificial-intelligence/2018/01/30/programmers-use-tensorflow-ai-to-turn-any-webcam-into-microsoft-kinect/\"],[\"Tech Radar\",\"https://www.techradar.com/news/ai-developers-can-turn-any-webcam-into-a-kinect\"],[\"GeekTime\",\"https://www.geektime.co.il/developers-create-kinect-with-tensorflow-and-webcam/\"],[\"Android Headlines\",\"https://www.androidheadlines.com/2018/01/tensorflow-unity-turn-webcams-into-ai-powered-ar-systems.html\"],[\"FossBytes\",\"https://fossbytes.com/programmers-transform-a-10-webcam-into-microsoft-kinect/\"]],\"components\":[[\"code\",\"C#, HLSL Python\"],[\"software\",\"Unity3D\"],[\"3d\",\"Tensorflow\"]],\"cover\":\"skeletron.jpg\",\"about\":\"Skeletron is a system that predicts joints and human skeleton position from real-time video taken by any RGB camera, such as a webcam. The system sends the data about the position of the human body to Unity, a 3D game development engine, to allow engineers, artists, and creative technologists to use it to develop digital experiences.\\nSkeletron was developed thanks and as a part of NYU ITP Xstory grant (Experiments in Storytelling).\",\"embed\":\"<iframe width=\\\"100%\\\" height=\\\"500\\\" src=\\\"https://www.youtube.com/embed/l_owi316cE8?rel=0&amp;controls=0&amp;showinfo=0\\\" frameborder=\\\"0\\\" allow=\\\"autoplay; encrypted-media\\\" allowfullscreen></iframe>\",\"excerpt\":\"Predict joints and human skeleton position from real-time video.\",\"tags\":[\"Machine Learning\",\"Tools\"]}}}}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/json-loader!./.cache/json/depthkit-js.json\n// module id = 379\n// module chunks = 121080360342820"],"sourceRoot":""}