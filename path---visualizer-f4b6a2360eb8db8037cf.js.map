{"version":3,"sources":["webpack:///path---visualizer-f4b6a2360eb8db8037cf.js","webpack:///./.cache/json/visualizer.json"],"names":["webpackJsonp","403","module","exports","data","markdownRemark","html","frontmatter","date","components","path","about","cover","credits","title","press","links","embed","tags","excerpt","pathContext","prev","id","next"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAkBC,MAAQC,gBAAkBC,KAAA,GAAAC,aAAyBC,KAAA,iBAAAC,aAAA,gGAAAC,KAAA,cAAAC,MAAA,0pBAAAC,MAAA,iBAAAC,QAAA,GAAAC,MAAA,aAAAC,SAAAC,QAAA,gLAAAC,MAAA,GAAAC,MAAA,cAAAC,QAAA,sCAA8oCC,aAAgBC,MAAQf,KAAA,GAAAgB,GAAA,+GAAAf,aAA6IC,KAAA,2BAAAE,KAAA,aAAAI,MAAA,YAAAE,QAAA,wFAAAH,QAAA,2IAAAE,QAAA,kjBAAAN,aAAA,sEAAAG,MAAA,gBAAAD,MAAA,ubAAAM,MAAA,oMAAqlDE,QAAA,mEAAAD,MAAA,8BAAgKK,MAASjB,KAAA,GAAAgB,GAAA,6GAAAf,aAA2IC,KAAA,2BAAAE,KAAA,WAAAI,MAAA,UAAAE,QAAA,+CAAAH,QAAA,iFAAAE,SAAAN,aAAA,gGAAAG,MAAA,cAAAD,MAAA,+dAAAM,MAAA,oMAA6/BE,QAAA,0BAAAD,MAAA","file":"path---visualizer-f4b6a2360eb8db8037cf.js","sourcesContent":["webpackJsonp([109302176427146],{\n\n/***/ 403:\n/***/ (function(module, exports) {\n\n\tmodule.exports = {\"data\":{\"markdownRemark\":{\"html\":\"\",\"frontmatter\":{\"date\":\"April 14, 2018\",\"components\":[[\"code\",\"Javascript, GLSL\"],[\"software\",\"Blender, e-on Vue, Adobe Photoshop\"],[\"3d\",\"Three.js\"]],\"path\":\"/visualizer\",\"about\":\"‘Visualizer’ was born after an R&D process I did with Roey Tsemah and Ronen Tanchum. Initially our goal was to research how we could use the Web Audio API, to analyse incoming signal (music or microphone inputs), and drive geometry deformations in real-time, while keeping performance optimal so it could run on mobile browsers as well.\\nAfter the R&D process was done, I went on to design the concept art for this visualizer, using 3D animation and environment design software packages and then implementing these designs in the web using Three.js. This work was made possible thanks to the amazing post “Experiments with Perlin Noise” examples by Jaume Sanchez.\",\"cover\":\"visualizer.png\",\"credits\":\"\",\"title\":\"Visualizer\",\"press\":[],\"links\":[[\"Demo\",\"https://juniorxsound.github.io/ICM-Fall-2016/3D_Web_Audio_Visualiser/\"],[\"Github\",\"https://github.com/juniorxsound/ICM-Fall-2016/tree/master/3D_Web_Audio_Visualiser\"]],\"embed\":\"\",\"tags\":[\"Experiment\"],\"excerpt\":\"A 3D web audio sound visulaizer.\"}}},\"pathContext\":{\"prev\":{\"html\":\"\",\"id\":\"/home/travis/build/juniorxsound/portfolio/src/pages/projects/skeletron.md absPath of file >>> MarkdownRemark\",\"frontmatter\":{\"date\":\"2018-04-15T22:12:03.284Z\",\"path\":\"/skeletron\",\"title\":\"Skeletron\",\"links\":[[\"Presskit\",\"https://drive.google.com/drive/folders/18uzf-grMetd9bZPNMHDNs7IZWZHNFmKY\"]],\"credits\":\"Developed with <a target=\\\"_blank\\\" href=\\\"https://drorayalon.com\\\">Dror Ayalon</a>. <br />Attribution: VNect ML Model VNect TensorFlow Port\",\"press\":[[\"The Next Web\",\"https://thenextweb.com/artificial-intelligence/2018/01/30/programmers-use-tensorflow-ai-to-turn-any-webcam-into-microsoft-kinect/\"],[\"Tech Radar\",\"https://www.techradar.com/news/ai-developers-can-turn-any-webcam-into-a-kinect\"],[\"GeekTime\",\"https://www.geektime.co.il/developers-create-kinect-with-tensorflow-and-webcam/\"],[\"Android Headlines\",\"https://www.androidheadlines.com/2018/01/tensorflow-unity-turn-webcams-into-ai-powered-ar-systems.html\"],[\"FossBytes\",\"https://fossbytes.com/programmers-transform-a-10-webcam-into-microsoft-kinect/\"]],\"components\":[[\"code\",\"C#, HLSL Python\"],[\"software\",\"Unity3D\"],[\"3d\",\"Tensorflow\"]],\"cover\":\"skeletron.jpg\",\"about\":\"Skeletron is a system that predicts joints and human skeleton position from real-time video taken by any RGB camera, such as a webcam. The system sends the data about the position of the human body to Unity, a 3D game development engine, to allow engineers, artists, and creative technologists to use it to develop digital experiences.\\nSkeletron was developed thanks and as a part of NYU ITP Xstory grant (Experiments in Storytelling).\",\"embed\":\"<iframe width=\\\"100%\\\" height=\\\"500\\\" src=\\\"https://www.youtube.com/embed/l_owi316cE8?rel=0&amp;controls=0&amp;showinfo=0\\\" frameborder=\\\"0\\\" allow=\\\"autoplay; encrypted-media\\\" allowfullscreen></iframe>\",\"excerpt\":\"Predict joints and human skeleton position from real-time video.\",\"tags\":[\"Machine Learning\",\"Tools\"]}},\"next\":{\"html\":\"\",\"id\":\"/home/travis/build/juniorxsound/portfolio/src/pages/projects/trumpet.md absPath of file >>> MarkdownRemark\",\"frontmatter\":{\"date\":\"2018-04-13T22:12:03.284Z\",\"path\":\"/trumpet\",\"title\":\"Trumpet\",\"links\":[[\"Github\",\"https://github.com/dodiku/trumpet\"]],\"credits\":\"Developed with <a target=\\\"_blank\\\" href=\\\"http://drorayalon.com\\\">Dror Ayalon</a>\",\"press\":[],\"components\":[[\"code\",\"Javascript, cSound\"],[\"software\",\"Node.js, cSound Node Bindings\"],[\"3d\",\"Twitter API\"]],\"cover\":\"trumpet.png\",\"about\":\"‘Trumpet’ is a Node.js server that listens to tweets from NYC that contain the words “trump” and “protest” and plays a note for every tweet. I developed ‘Trumpet’ with Dror Ayalon during Spotify’s NYC Monthly Music Hackathon.\\nSince the Hackathon took place on January 21st, 2017, a day after Donald Trump’s inauguration, we knew we wanted to create a generative music composition based on people emotions towards the president elect, and the protests around the inauguration.\",\"embed\":\"<iframe width=\\\"100%\\\" height=\\\"450\\\" src=\\\"https://www.youtube.com/embed/4YlOzWwsXKo?rel=0&amp;controls=0&amp;showinfo=0\\\" frameborder=\\\"0\\\" allow=\\\"autoplay; encrypted-media\\\" allowfullscreen></iframe>\",\"excerpt\":\"The sound of a protest.\",\"tags\":[\"Experiment\"]}}}}\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// path---visualizer-f4b6a2360eb8db8037cf.js","module.exports = {\"data\":{\"markdownRemark\":{\"html\":\"\",\"frontmatter\":{\"date\":\"April 14, 2018\",\"components\":[[\"code\",\"Javascript, GLSL\"],[\"software\",\"Blender, e-on Vue, Adobe Photoshop\"],[\"3d\",\"Three.js\"]],\"path\":\"/visualizer\",\"about\":\"‘Visualizer’ was born after an R&D process I did with Roey Tsemah and Ronen Tanchum. Initially our goal was to research how we could use the Web Audio API, to analyse incoming signal (music or microphone inputs), and drive geometry deformations in real-time, while keeping performance optimal so it could run on mobile browsers as well.\\nAfter the R&D process was done, I went on to design the concept art for this visualizer, using 3D animation and environment design software packages and then implementing these designs in the web using Three.js. This work was made possible thanks to the amazing post “Experiments with Perlin Noise” examples by Jaume Sanchez.\",\"cover\":\"visualizer.png\",\"credits\":\"\",\"title\":\"Visualizer\",\"press\":[],\"links\":[[\"Demo\",\"https://juniorxsound.github.io/ICM-Fall-2016/3D_Web_Audio_Visualiser/\"],[\"Github\",\"https://github.com/juniorxsound/ICM-Fall-2016/tree/master/3D_Web_Audio_Visualiser\"]],\"embed\":\"\",\"tags\":[\"Experiment\"],\"excerpt\":\"A 3D web audio sound visulaizer.\"}}},\"pathContext\":{\"prev\":{\"html\":\"\",\"id\":\"/home/travis/build/juniorxsound/portfolio/src/pages/projects/skeletron.md absPath of file >>> MarkdownRemark\",\"frontmatter\":{\"date\":\"2018-04-15T22:12:03.284Z\",\"path\":\"/skeletron\",\"title\":\"Skeletron\",\"links\":[[\"Presskit\",\"https://drive.google.com/drive/folders/18uzf-grMetd9bZPNMHDNs7IZWZHNFmKY\"]],\"credits\":\"Developed with <a target=\\\"_blank\\\" href=\\\"https://drorayalon.com\\\">Dror Ayalon</a>. <br />Attribution: VNect ML Model VNect TensorFlow Port\",\"press\":[[\"The Next Web\",\"https://thenextweb.com/artificial-intelligence/2018/01/30/programmers-use-tensorflow-ai-to-turn-any-webcam-into-microsoft-kinect/\"],[\"Tech Radar\",\"https://www.techradar.com/news/ai-developers-can-turn-any-webcam-into-a-kinect\"],[\"GeekTime\",\"https://www.geektime.co.il/developers-create-kinect-with-tensorflow-and-webcam/\"],[\"Android Headlines\",\"https://www.androidheadlines.com/2018/01/tensorflow-unity-turn-webcams-into-ai-powered-ar-systems.html\"],[\"FossBytes\",\"https://fossbytes.com/programmers-transform-a-10-webcam-into-microsoft-kinect/\"]],\"components\":[[\"code\",\"C#, HLSL Python\"],[\"software\",\"Unity3D\"],[\"3d\",\"Tensorflow\"]],\"cover\":\"skeletron.jpg\",\"about\":\"Skeletron is a system that predicts joints and human skeleton position from real-time video taken by any RGB camera, such as a webcam. The system sends the data about the position of the human body to Unity, a 3D game development engine, to allow engineers, artists, and creative technologists to use it to develop digital experiences.\\nSkeletron was developed thanks and as a part of NYU ITP Xstory grant (Experiments in Storytelling).\",\"embed\":\"<iframe width=\\\"100%\\\" height=\\\"500\\\" src=\\\"https://www.youtube.com/embed/l_owi316cE8?rel=0&amp;controls=0&amp;showinfo=0\\\" frameborder=\\\"0\\\" allow=\\\"autoplay; encrypted-media\\\" allowfullscreen></iframe>\",\"excerpt\":\"Predict joints and human skeleton position from real-time video.\",\"tags\":[\"Machine Learning\",\"Tools\"]}},\"next\":{\"html\":\"\",\"id\":\"/home/travis/build/juniorxsound/portfolio/src/pages/projects/trumpet.md absPath of file >>> MarkdownRemark\",\"frontmatter\":{\"date\":\"2018-04-13T22:12:03.284Z\",\"path\":\"/trumpet\",\"title\":\"Trumpet\",\"links\":[[\"Github\",\"https://github.com/dodiku/trumpet\"]],\"credits\":\"Developed with <a target=\\\"_blank\\\" href=\\\"http://drorayalon.com\\\">Dror Ayalon</a>\",\"press\":[],\"components\":[[\"code\",\"Javascript, cSound\"],[\"software\",\"Node.js, cSound Node Bindings\"],[\"3d\",\"Twitter API\"]],\"cover\":\"trumpet.png\",\"about\":\"‘Trumpet’ is a Node.js server that listens to tweets from NYC that contain the words “trump” and “protest” and plays a note for every tweet. I developed ‘Trumpet’ with Dror Ayalon during Spotify’s NYC Monthly Music Hackathon.\\nSince the Hackathon took place on January 21st, 2017, a day after Donald Trump’s inauguration, we knew we wanted to create a generative music composition based on people emotions towards the president elect, and the protests around the inauguration.\",\"embed\":\"<iframe width=\\\"100%\\\" height=\\\"450\\\" src=\\\"https://www.youtube.com/embed/4YlOzWwsXKo?rel=0&amp;controls=0&amp;showinfo=0\\\" frameborder=\\\"0\\\" allow=\\\"autoplay; encrypted-media\\\" allowfullscreen></iframe>\",\"excerpt\":\"The sound of a protest.\",\"tags\":[\"Experiment\"]}}}}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/json-loader!./.cache/json/visualizer.json\n// module id = 403\n// module chunks = 109302176427146"],"sourceRoot":""}