webpackJsonp([26983603911290],{378:function(e,t){e.exports={data:{markdownRemark:{html:"",frontmatter:{date:"April 23, 2018",components:[["code","Python, C#, HLSL"],["software","Unity3D, Tensorflow"],["3d","ARKit, Volume"]],path:"/pulp-fiction",about:"Inside Pulp Fiction is an experiment that uses machine learning to reconstruct Pulp Fiction's iconic dance scene in Augmented Reality. The experiment is a part of Volume, a machine learning driven tool to reconstruct 3D models from 2D images and video.",cover:"pulp_cover.jpg",credits:'Developed with <a target="_blank" href="https://shirin.works">~shirin anlen</a>',title:"Inside Pulp Fiction",press:[["The Next Web","https://thenextweb.com/artificial-intelligence/2018/01/22/ai-rips-objects-from-video-and-reimagines-them-in-3d-ar/"],["Discovery Channel","https://www.youtube.com/watch?v=Zi4yof2yy04"],["Vice","https://motherboard.vice.com/en_us/article/gywamy/cue-up-the-pulp-fiction-dance-scene-this-app-3d-projects-2d-movies-in-your-living-room"],["Mashable","http://mashable.france24.com/tech-business/20180130-films-volume-realite-augmentee-cinema-technologie"],["UploadVR","https://uploadvr.com/ar-app-brings-pulp-fiction-characters-living-room/"],["prosthetic knowledge","http://prostheticknowledge.tumblr.com/post/170014746561/volume-in-development-project-from-or-fleisher"],["VRFocus","https://www.vrfocus.com/2018/01/reconstruct-your-favourite-movie-in-ar/"],["Android Headlines","https://www.androidheadlines.com/2018/01/volume-ai-program-puts-2d-objects-3d-spaces.html"],["Labroots","https://www.labroots.com/trending/videos/11371/ai-tool-turns-video-into-3d-augmented-reality-experiences"]],links:[["Website","https://volume.gl"],["Github","https://github.com/Volume-GL/Pulp-Fiction-ARKit"],["Presskit","https://drive.google.com/drive/folders/1XBQgptNAchJr0kUSD0LhzUzxdKnZ4Rud"],["Presentation","https://vimeo.com/270479574"]],embed:'<iframe width="100%" height="450" src="https://www.youtube.com/embed/iwJt4DM6mJA?rel=0&amp;controls=0&amp;showinfo=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>',tags:["Augmented Reality","Machine Learning"],excerpt:"Step inside Pulp Fiction's iconic dance scene using augmented reality."}}},pathContext:{prev:{html:"",id:"/home/travis/build/juniorxsound/portfolio/src/pages/projects/myth.md absPath of file >>> MarkdownRemark",frontmatter:{date:"2018-05-23T12:34:00+00:00",path:"/myth",title:"Myth",links:[["Full Experience","http://film.livyatanim.com"],["Album","https://livyatanim.bandcamp.com"],["Presskit","http://film.livyatanim.com/media/mediakit.zip"]],credits:"Developed with Yannis Gravezas, Tomer Rousso and Livyatanim",press:[["Wired","https://www.wired.de/collection/life/10-virtual-reality-filme-die-man-gesehen-haben-muss"],["Creators Project","https://creators.vice.com/en_us/article/ez5qva/float-through-a-virtual-world-of-hybrid-beings-in-myth"],["We and the Color","https://weandthecolor.com/webgl-short-film-livyatanim-myth/62302"],["prosthetic knowledge","http://prostheticknowledge.tumblr.com/post/133824524661/myth-interactive-web-music-video-for-livyatanim'"],["Z","http://z.ultranoir.com/en/articles/1282-livyatanim-myth-a-vr-film-by-or-fleisher.html"],["Chrome Experiments","https://experiments.withgoogle.com/livyatanim-myth"],["WorldFest- NASA Remi Award winner","#"],["UrbamMediaMakers Best Interactive Award Winner","#"],["The FWA – WOTD","#"],["CSS Awards – WOTD","#"],["Awwwards – Honorable Mention","#"]],components:[["code","Javascript, GLSL"],["software","Blender, e-on Vue, Adobe Photoshop, Autodesk Maya and Ableton Live, Web Audio API, Web MIDI API & WebVR API."],["3d","Three.js"]],cover:"myth-cover.png",about:"‘Myth’, is an interactive web virtual reality short film, featuring the song “Can I peacfuly Love” from Livyatanim’s debut album “After the Waters”. The film takes place in a dark surreal world, which aims to blur the lines between digital and natural imagery.\nThe film uses the composition’s notation, rhythms and melodies (MIDI), to control elements ranging from drums affecting the geometry to transitions between scenes. In effect, using this data transformed from being a musical composition language, to a visual directing language.\nThe experience can be watched on a wide range of platforms from desktop computers, mobile phones and VR headsets.",embed:'<div style="padding:56.25% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/145578640?autoplay=0&title=0&byline=0&portrait=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div><script src="https://player.vimeo.com/api/player.js"></script>',excerpt:"An audio reactive virtual reality short film.",tags:["Virtual Reality"]}},next:{html:"",id:"/home/travis/build/juniorxsound/portfolio/src/pages/projects/retouch.md absPath of file >>> MarkdownRemark",frontmatter:{date:"2018-04-23T12:34:00+00:02",path:"/retouch",title:"ReTouch",links:[["Github","https://github.com/juniorxsound/ReTouch"]],credits:"Developed by under the advisement of Prof. Ken Perlin and Prof. Daniele Panozzo @ Computer Science Department, New York University",press:[],components:[["code","C++, GLSL"],["software","Volume"],["3d","OpenGL"]],cover:"shining.jpg",about:"ReTouch is an OpenGL application that enables editing and retouching of images using depth-maps in 2.5D. The depth maps are generated by Volume, a state of the art tool, that uses a CNN (Convolutional Neural Network) to predict depth-maps from 2D images . ReTouch uses these depth-maps to enable the addition of depth of field and color retouching for the foreground and background separately.",embed:'<iframe width="100%" height="450" src="https://www.youtube.com/embed/CAsy_jm85ZY?rel=0&amp;controls=0&amp;showinfo=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>',excerpt:"Edit and retouch any image in 2.5D.",tags:["Machine Learning","Tools"]}}}}}});
//# sourceMappingURL=path---pulp-fiction-d847b224f8ab9e456fd7.js.map