{"version":3,"sources":["webpack:///path---retouch-b0c89cdf9b249e09799a.js","webpack:///./.cache/json/retouch.json"],"names":["webpackJsonp","388","module","exports","data","markdownRemark","html","frontmatter","date","components","path","about","cover","credits","title","press","links","embed","tags","excerpt","pathContext","prev","id","next"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAkBC,MAAQC,gBAAkBC,KAAA,GAAAC,aAAyBC,KAAA,gBAAAC,aAAA,2DAAAC,KAAA,WAAAC,MAAA,4YAAAC,MAAA,cAAAC,QAAA,qIAAAC,MAAA,UAAAC,SAAAC,QAAA,qDAAAC,MAAA,oMAAg7BC,MAAA,4BAAAC,QAAA,yCAAoIC,aAAgBC,MAAQf,KAAA,GAAAgB,GAAA,0GAAAf,aAAwIC,KAAA,4BAAAE,KAAA,QAAAI,MAAA,OAAAE,QAAA,4NAAAH,QAAA,4EAAAE,QAAA,ySAAAN,aAAA,wKAAAG,MAAA,WAAAD,MAAA,koBAAAM,MAAA,oMAA4qDE,QAAA,mCAAAD,MAAA,qBAAuHK,MAASjB,KAAA,GAAAgB,GAAA,+GAAAf,aAA6IC,KAAA,4BAAAE,KAAA,aAAAI,MAAA,YAAAE,QAAA,wFAAAH,QAAA,2IAAAE,QAAA,kjBAAAN,aAAA,sEAAAG,MAAA,gBAAAD,MAAA,ubAAAM,MAAA,oMAAslDE,QAAA,mEAAAD,MAAA","file":"path---retouch-b0c89cdf9b249e09799a.js","sourcesContent":["webpackJsonp([262858279625854],{\n\n/***/ 388:\n/***/ (function(module, exports) {\n\n\tmodule.exports = {\"data\":{\"markdownRemark\":{\"html\":\"\",\"frontmatter\":{\"date\":\"June 18, 2018\",\"components\":[[\"code\",\"C++, GLSL\"],[\"software\",\"Volume\"],[\"3d\",\"OpenGL\"]],\"path\":\"/retouch\",\"about\":\"ReTouch is an OpenGL application that enables editing and retouching of images using depth-maps in 2.5D. The depth maps are generated by Volume, a state of the art tool, that uses a CNN (Convolutional Neural Network) to predict depth-maps from 2D images . ReTouch uses these depth-maps to enable the addition of depth of field and color retouching for the foreground and background separately.\",\"cover\":\"shining.jpg\",\"credits\":\"Developed by under the advisement of Prof. Ken Perlin and Prof. Daniele Panozzo @ Computer Science Department, New York University\",\"title\":\"ReTouch\",\"press\":[],\"links\":[[\"Github\",\"https://github.com/juniorxsound/ReTouch\"]],\"embed\":\"<iframe width=\\\"100%\\\" height=\\\"450\\\" src=\\\"https://www.youtube.com/embed/CAsy_jm85ZY?rel=0&amp;controls=0&amp;showinfo=0\\\" frameborder=\\\"0\\\" allow=\\\"autoplay; encrypted-media\\\" allowfullscreen></iframe>\",\"tags\":[\"Machine Learning\",\"Tools\"],\"excerpt\":\"Edit and retouch any image in 2.5D.\"}}},\"pathContext\":{\"prev\":{\"html\":\"\",\"id\":\"/home/travis/build/juniorxsound/portfolio/src/pages/projects/sono.md absPath of file >>> MarkdownRemark\",\"frontmatter\":{\"date\":\"2018-06-19T12:29:00+00:00\",\"path\":\"/sono\",\"title\":\"Sono\",\"links\":[[\"Full Experience\",\"http://sono.livyatanim.com/\"],[\"Album\",\"https://livyatanim.bandcamp.com\"],[\"Presskit\",\"http://sono.livyatanim.com/media/sono_mediakit.zip\"],[\"Making-of\",\"https://www.youtube.com/watch?v=5_0eb7B9yoo\"]],\"credits\":\"Developed with Yannis Gravezas, Ronen Tanchum, Ilya Marcus and Livyatanim\",\"press\":[[\"Creators Project\",\"https://creators.vice.com/en_us/article/aenxpb/sono-livyatanim-audio-reactive-live-vr-performance\"],[\"WebVR Experiments with Google\",\"https://experiments.withgoogle.com/sono\"],[\"VRRoom\",\"https://www.vrroom.buzz/vr-news/immersive-arts/cosmic-visuals-react-live-audio-vr-show\"]],\"components\":[[\"code\",\"Javascript, GLSL\"],[\"software\",\"Blender, e-on Vue, TouchDesigner, Autodesk Maya and Ableton Live, Web Audio API, Web MIDI API & WebVR API.\"],[\"3d\",\"Three.js\"]],\"cover\":\"sono.png\",\"about\":\"‘SONO’ is a binaural webVR musical performance featuring music from Livyatanim’s debut album ‘After the Waters’.\\nThe ‘venue’ in which the band plays is a dark crater located in a surreal outer-space environment, surrounded by cosmic events and astronomical phenomenons. ‘SONO’ features three songs, each of them played by the band as the surrounding world changes around them. The music, like the visuals – is binaural, allowing the audience to move around and hear what they would hear if they were surrounded by the band.\\nThe experience can be watched on a wide range of platforms from desktop computers, mobile phones and VR headsets.\",\"embed\":\"<iframe width=\\\"100%\\\" height=\\\"450\\\" src=\\\"https://www.youtube.com/embed/e30AUS9HFtE?rel=0&amp;controls=1&amp;showinfo=0\\\" frameborder=\\\"0\\\" allow=\\\"autoplay; encrypted-media\\\" allowfullscreen></iframe>\",\"excerpt\":\"A cosmic webVR music performance\",\"tags\":[\"Virtual Reality\"]}},\"next\":{\"html\":\"\",\"id\":\"/home/travis/build/juniorxsound/portfolio/src/pages/projects/skeletron.md absPath of file >>> MarkdownRemark\",\"frontmatter\":{\"date\":\"2018-06-17T12:23:00+00:02\",\"path\":\"/skeletron\",\"title\":\"Skeletron\",\"links\":[[\"Presskit\",\"https://drive.google.com/drive/folders/18uzf-grMetd9bZPNMHDNs7IZWZHNFmKY\"]],\"credits\":\"Developed with <a target=\\\"_blank\\\" href=\\\"https://drorayalon.com\\\">Dror Ayalon</a>. <br />Attribution: VNect ML Model VNect TensorFlow Port\",\"press\":[[\"The Next Web\",\"https://thenextweb.com/artificial-intelligence/2018/01/30/programmers-use-tensorflow-ai-to-turn-any-webcam-into-microsoft-kinect/\"],[\"Tech Radar\",\"https://www.techradar.com/news/ai-developers-can-turn-any-webcam-into-a-kinect\"],[\"GeekTime\",\"https://www.geektime.co.il/developers-create-kinect-with-tensorflow-and-webcam/\"],[\"Android Headlines\",\"https://www.androidheadlines.com/2018/01/tensorflow-unity-turn-webcams-into-ai-powered-ar-systems.html\"],[\"FossBytes\",\"https://fossbytes.com/programmers-transform-a-10-webcam-into-microsoft-kinect/\"]],\"components\":[[\"code\",\"C#, HLSL Python\"],[\"software\",\"Unity3D\"],[\"3d\",\"Tensorflow\"]],\"cover\":\"skeletron.jpg\",\"about\":\"Skeletron is a system that predicts joints and human skeleton position from real-time video taken by any RGB camera, such as a webcam. The system sends the data about the position of the human body to Unity, a 3D game development engine, to allow engineers, artists, and creative technologists to use it to develop digital experiences.\\nSkeletron was developed thanks and as a part of NYU ITP Xstory grant (Experiments in Storytelling).\",\"embed\":\"<iframe width=\\\"100%\\\" height=\\\"500\\\" src=\\\"https://www.youtube.com/embed/l_owi316cE8?rel=0&amp;controls=0&amp;showinfo=0\\\" frameborder=\\\"0\\\" allow=\\\"autoplay; encrypted-media\\\" allowfullscreen></iframe>\",\"excerpt\":\"Predict joints and human skeleton position from real-time video.\",\"tags\":[\"Machine Learning\",\"Tools\"]}}}}\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// path---retouch-b0c89cdf9b249e09799a.js","module.exports = {\"data\":{\"markdownRemark\":{\"html\":\"\",\"frontmatter\":{\"date\":\"June 18, 2018\",\"components\":[[\"code\",\"C++, GLSL\"],[\"software\",\"Volume\"],[\"3d\",\"OpenGL\"]],\"path\":\"/retouch\",\"about\":\"ReTouch is an OpenGL application that enables editing and retouching of images using depth-maps in 2.5D. The depth maps are generated by Volume, a state of the art tool, that uses a CNN (Convolutional Neural Network) to predict depth-maps from 2D images . ReTouch uses these depth-maps to enable the addition of depth of field and color retouching for the foreground and background separately.\",\"cover\":\"shining.jpg\",\"credits\":\"Developed by under the advisement of Prof. Ken Perlin and Prof. Daniele Panozzo @ Computer Science Department, New York University\",\"title\":\"ReTouch\",\"press\":[],\"links\":[[\"Github\",\"https://github.com/juniorxsound/ReTouch\"]],\"embed\":\"<iframe width=\\\"100%\\\" height=\\\"450\\\" src=\\\"https://www.youtube.com/embed/CAsy_jm85ZY?rel=0&amp;controls=0&amp;showinfo=0\\\" frameborder=\\\"0\\\" allow=\\\"autoplay; encrypted-media\\\" allowfullscreen></iframe>\",\"tags\":[\"Machine Learning\",\"Tools\"],\"excerpt\":\"Edit and retouch any image in 2.5D.\"}}},\"pathContext\":{\"prev\":{\"html\":\"\",\"id\":\"/home/travis/build/juniorxsound/portfolio/src/pages/projects/sono.md absPath of file >>> MarkdownRemark\",\"frontmatter\":{\"date\":\"2018-06-19T12:29:00+00:00\",\"path\":\"/sono\",\"title\":\"Sono\",\"links\":[[\"Full Experience\",\"http://sono.livyatanim.com/\"],[\"Album\",\"https://livyatanim.bandcamp.com\"],[\"Presskit\",\"http://sono.livyatanim.com/media/sono_mediakit.zip\"],[\"Making-of\",\"https://www.youtube.com/watch?v=5_0eb7B9yoo\"]],\"credits\":\"Developed with Yannis Gravezas, Ronen Tanchum, Ilya Marcus and Livyatanim\",\"press\":[[\"Creators Project\",\"https://creators.vice.com/en_us/article/aenxpb/sono-livyatanim-audio-reactive-live-vr-performance\"],[\"WebVR Experiments with Google\",\"https://experiments.withgoogle.com/sono\"],[\"VRRoom\",\"https://www.vrroom.buzz/vr-news/immersive-arts/cosmic-visuals-react-live-audio-vr-show\"]],\"components\":[[\"code\",\"Javascript, GLSL\"],[\"software\",\"Blender, e-on Vue, TouchDesigner, Autodesk Maya and Ableton Live, Web Audio API, Web MIDI API & WebVR API.\"],[\"3d\",\"Three.js\"]],\"cover\":\"sono.png\",\"about\":\"‘SONO’ is a binaural webVR musical performance featuring music from Livyatanim’s debut album ‘After the Waters’.\\nThe ‘venue’ in which the band plays is a dark crater located in a surreal outer-space environment, surrounded by cosmic events and astronomical phenomenons. ‘SONO’ features three songs, each of them played by the band as the surrounding world changes around them. The music, like the visuals – is binaural, allowing the audience to move around and hear what they would hear if they were surrounded by the band.\\nThe experience can be watched on a wide range of platforms from desktop computers, mobile phones and VR headsets.\",\"embed\":\"<iframe width=\\\"100%\\\" height=\\\"450\\\" src=\\\"https://www.youtube.com/embed/e30AUS9HFtE?rel=0&amp;controls=1&amp;showinfo=0\\\" frameborder=\\\"0\\\" allow=\\\"autoplay; encrypted-media\\\" allowfullscreen></iframe>\",\"excerpt\":\"A cosmic webVR music performance\",\"tags\":[\"Virtual Reality\"]}},\"next\":{\"html\":\"\",\"id\":\"/home/travis/build/juniorxsound/portfolio/src/pages/projects/skeletron.md absPath of file >>> MarkdownRemark\",\"frontmatter\":{\"date\":\"2018-06-17T12:23:00+00:02\",\"path\":\"/skeletron\",\"title\":\"Skeletron\",\"links\":[[\"Presskit\",\"https://drive.google.com/drive/folders/18uzf-grMetd9bZPNMHDNs7IZWZHNFmKY\"]],\"credits\":\"Developed with <a target=\\\"_blank\\\" href=\\\"https://drorayalon.com\\\">Dror Ayalon</a>. <br />Attribution: VNect ML Model VNect TensorFlow Port\",\"press\":[[\"The Next Web\",\"https://thenextweb.com/artificial-intelligence/2018/01/30/programmers-use-tensorflow-ai-to-turn-any-webcam-into-microsoft-kinect/\"],[\"Tech Radar\",\"https://www.techradar.com/news/ai-developers-can-turn-any-webcam-into-a-kinect\"],[\"GeekTime\",\"https://www.geektime.co.il/developers-create-kinect-with-tensorflow-and-webcam/\"],[\"Android Headlines\",\"https://www.androidheadlines.com/2018/01/tensorflow-unity-turn-webcams-into-ai-powered-ar-systems.html\"],[\"FossBytes\",\"https://fossbytes.com/programmers-transform-a-10-webcam-into-microsoft-kinect/\"]],\"components\":[[\"code\",\"C#, HLSL Python\"],[\"software\",\"Unity3D\"],[\"3d\",\"Tensorflow\"]],\"cover\":\"skeletron.jpg\",\"about\":\"Skeletron is a system that predicts joints and human skeleton position from real-time video taken by any RGB camera, such as a webcam. The system sends the data about the position of the human body to Unity, a 3D game development engine, to allow engineers, artists, and creative technologists to use it to develop digital experiences.\\nSkeletron was developed thanks and as a part of NYU ITP Xstory grant (Experiments in Storytelling).\",\"embed\":\"<iframe width=\\\"100%\\\" height=\\\"500\\\" src=\\\"https://www.youtube.com/embed/l_owi316cE8?rel=0&amp;controls=0&amp;showinfo=0\\\" frameborder=\\\"0\\\" allow=\\\"autoplay; encrypted-media\\\" allowfullscreen></iframe>\",\"excerpt\":\"Predict joints and human skeleton position from real-time video.\",\"tags\":[\"Machine Learning\",\"Tools\"]}}}}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/json-loader!./.cache/json/retouch.json\n// module id = 388\n// module chunks = 262858279625854"],"sourceRoot":""}