webpackJsonp([0xef117662247e],{382:function(e,t){e.exports={data:{markdownRemark:{html:"",frontmatter:{date:"April 23, 2018",components:[["code","C++, GLSL"],["software","Volume"],["3d","OpenGL"]],path:"/retouch",about:"ReTouch is an OpenGL application that enables editing and retouching of images using depth-maps in 2.5D. The depth maps are generated by Volume, a state of the art tool, that uses a CNN (Convolutional Neural Network) to predict depth-maps from 2D images . ReTouch uses these depth-maps to enable the addition of depth of field and color retouching for the foreground and background separately.",cover:"shining.jpg",credits:"Developed by under the advisement of Prof. Ken Perlin and Prof. Daniele Panozzo @ Computer Science Department, New York University",title:"ReTouch",press:[],links:[["Github","https://github.com/juniorxsound/ReTouch"]],embed:'<iframe width="100%" height="450" src="https://www.youtube.com/embed/CAsy_jm85ZY?rel=0&amp;controls=0&amp;showinfo=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>',tags:["Machine Learning","Tools"],excerpt:"Edit and retouch any image in 2.5D."}}},pathContext:{prev:{html:"",id:"/home/travis/build/juniorxsound/portfolio/src/pages/projects/pulpfiction.md absPath of file >>> MarkdownRemark",frontmatter:{date:"2018-04-23T12:34:00+00:02",path:"/pulp-fiction",title:"Inside Pulp Fiction",links:[["Website","https://volume.gl"],["Github","https://github.com/Volume-GL/Pulp-Fiction-ARKit"],["Presskit","https://drive.google.com/drive/folders/1XBQgptNAchJr0kUSD0LhzUzxdKnZ4Rud"],["Presentation","https://vimeo.com/270479574"]],credits:'Developed with <a target="_blank" href="https://shirin.works">~shirin anlen</a>',press:[["The Next Web","https://thenextweb.com/artificial-intelligence/2018/01/22/ai-rips-objects-from-video-and-reimagines-them-in-3d-ar/"],["Discovery Channel","https://www.youtube.com/watch?v=Zi4yof2yy04"],["Vice","https://motherboard.vice.com/en_us/article/gywamy/cue-up-the-pulp-fiction-dance-scene-this-app-3d-projects-2d-movies-in-your-living-room"],["Mashable","http://mashable.france24.com/tech-business/20180130-films-volume-realite-augmentee-cinema-technologie"],["UploadVR","https://uploadvr.com/ar-app-brings-pulp-fiction-characters-living-room/"],["prosthetic knowledge","http://prostheticknowledge.tumblr.com/post/170014746561/volume-in-development-project-from-or-fleisher"],["VRFocus","https://www.vrfocus.com/2018/01/reconstruct-your-favourite-movie-in-ar/"],["Android Headlines","https://www.androidheadlines.com/2018/01/volume-ai-program-puts-2d-objects-3d-spaces.html"],["Labroots","https://www.labroots.com/trending/videos/11371/ai-tool-turns-video-into-3d-augmented-reality-experiences"]],components:[["code","Python, C#, HLSL"],["software","Unity3D, Tensorflow"],["3d","ARKit, Volume"]],cover:"pulp_cover.jpg",about:"Inside Pulp Fiction is an experiment that uses machine learning to reconstruct Pulp Fiction's iconic dance scene in Augmented Reality. The experiment is a part of Volume, a machine learning driven tool to reconstruct 3D models from 2D images and video.",embed:'<iframe width="100%" height="450" src="https://www.youtube.com/embed/iwJt4DM6mJA?rel=0&amp;controls=0&amp;showinfo=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>',excerpt:"Step inside Pulp Fiction's iconic dance scene using augmented reality.",tags:["Augmented Reality","Machine Learning"]}},next:{html:"",id:"/home/travis/build/juniorxsound/portfolio/src/pages/projects/skeletron.md absPath of file >>> MarkdownRemark",frontmatter:{date:"2018-04-23T12:34:00+00:02",path:"/skeletron",title:"Skeletron",links:[["Presskit","https://drive.google.com/drive/folders/18uzf-grMetd9bZPNMHDNs7IZWZHNFmKY"]],credits:'Developed with <a target="_blank" href="https://drorayalon.com">Dror Ayalon</a>. <br />Attribution: VNect ML Model VNect TensorFlow Port',press:[["The Next Web","https://thenextweb.com/artificial-intelligence/2018/01/30/programmers-use-tensorflow-ai-to-turn-any-webcam-into-microsoft-kinect/"],["Tech Radar","https://www.techradar.com/news/ai-developers-can-turn-any-webcam-into-a-kinect"],["GeekTime","https://www.geektime.co.il/developers-create-kinect-with-tensorflow-and-webcam/"],["Android Headlines","https://www.androidheadlines.com/2018/01/tensorflow-unity-turn-webcams-into-ai-powered-ar-systems.html"],["FossBytes","https://fossbytes.com/programmers-transform-a-10-webcam-into-microsoft-kinect/"]],components:[["code","C#, HLSL Python"],["software","Unity3D"],["3d","Tensorflow"]],cover:"skeletron.jpg",about:"Skeletron is a system that predicts joints and human skeleton position from real-time video taken by any RGB camera, such as a webcam. The system sends the data about the position of the human body to Unity, a 3D game development engine, to allow engineers, artists, and creative technologists to use it to develop digital experiences.\nSkeletron was developed thanks and as a part of NYU ITP Xstory grant (Experiments in Storytelling).",embed:'<iframe width="100%" height="500" src="https://www.youtube.com/embed/l_owi316cE8?rel=0&amp;controls=0&amp;showinfo=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>',excerpt:"Predict joints and human skeleton position from real-time video.",tags:["Machine Learning","Tools"]}}}}}});
//# sourceMappingURL=path---retouch-da1b51eac72ff7eca778.js.map