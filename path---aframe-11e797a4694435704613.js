webpackJsonp([0xbd45de674942],{377:function(e,t){e.exports={data:{markdownRemark:{html:"",frontmatter:{date:"April 16, 2018",components:[["code","Javascript, GLSL"],["software","DepthKit"],["3d","AFrame"]],path:"/aframe",about:"An A-Frame component for rendering Volumetric videos captured using DepthKit (i.e Kinect + DSLR) in WebVR. The component wraps DepthKit.js which provides a similar interface for Three.js projects.",cover:"aframe_cover.jpg",credits:"",title:"DepthKit for AFrame",press:[],links:[["Github","https://github.com/juniorxsound/DepthKit-A-Frame"],["npm package","https://www.npmjs.com/package/aframe-depthkit"]],embed:"",tags:["Virtual Reality","Tools"],excerpt:"An AFrame component for rendering volumetric video in WebVR"}}},pathContext:{prev:{html:"",id:"/home/travis/build/juniorxsound/portfolio/src/pages/projects/detune.md absPath of file >>> MarkdownRemark",frontmatter:{date:"2018-04-17T22:12:03.284Z",path:"/detune",title:"detune",links:[["Website","https://detune.app"],["App store","https://itunes.apple.com/us/app/detune-music-with-your-face/id1370740132?mt=8"],["Github","https://github.com/dodiku/detune"],["Presskit","http://www.detuneapp.com/media.html"]],credits:'Developed with <a target="_blank" href="https://drorayalon.com">Dror Ayalon</a>',press:[["AudioKit","https://audiokitpro.com/detune-play-music-with-your-face/"]],components:[["code","Swift"],["software","AudioKit"],["3d","iPhoneX TrueDepth"]],cover:"detune_cover.png",about:"detune uses Apple’s ARKit and the TrueDepth camera (currently available only on iPhone X) to trigger music events, and to allow users to play music using face impressions. The motivation behind the project was to make Apple’s TrueDepth camera more accessible for creative coders.",embed:'<iframe width="100%" height="315" src="https://www.youtube.com/embed/7xnZwB00mrE?rel=0&amp;controls=0&amp;showinfo=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>',excerpt:"detune allows users to play music using face impressions.",tags:["Experiments","Tools"]}},next:{html:"",id:"/home/travis/build/juniorxsound/portfolio/src/pages/projects/skeletron.md absPath of file >>> MarkdownRemark",frontmatter:{date:"2018-04-15T22:12:03.284Z",path:"/skeletron",title:"Skeletron",links:[["Presskit","https://drive.google.com/drive/folders/18uzf-grMetd9bZPNMHDNs7IZWZHNFmKY"]],credits:'Developed with <a target="_blank" href="https://drorayalon.com">Dror Ayalon</a>. <br />Attribution: VNect ML Model VNect TensorFlow Port',press:[["The Next Web","https://thenextweb.com/artificial-intelligence/2018/01/30/programmers-use-tensorflow-ai-to-turn-any-webcam-into-microsoft-kinect/"],["Tech Radar","https://www.techradar.com/news/ai-developers-can-turn-any-webcam-into-a-kinect"],["GeekTime","https://www.geektime.co.il/developers-create-kinect-with-tensorflow-and-webcam/"],["Android Headlines","https://www.androidheadlines.com/2018/01/tensorflow-unity-turn-webcams-into-ai-powered-ar-systems.html"],["FossBytes","https://fossbytes.com/programmers-transform-a-10-webcam-into-microsoft-kinect/"]],components:[["code","C#, HLSL Python"],["software","Unity3D"],["3d","Tensorflow"]],cover:"skeletron.jpg",about:"Skeletron is a system that predicts joints and human skeleton position from real-time video taken by any RGB camera, such as a webcam. The system sends the data about the position of the human body to Unity, a 3D game development engine, to allow engineers, artists, and creative technologists to use it to develop digital experiences.\nSkeletron was developed thanks and as a part of NYU ITP Xstory grant (Experiments in Storytelling).",embed:'<iframe width="100%" height="500" src="https://www.youtube.com/embed/l_owi316cE8?rel=0&amp;controls=0&amp;showinfo=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>',excerpt:"Predict joints and human skeleton position from real-time video.",tags:["Machine Learning","Tools"]}}}}}});
//# sourceMappingURL=path---aframe-11e797a4694435704613.js.map