webpackJsonp([68671615135808],{400:function(t,e){t.exports={data:{markdownRemark:{html:"",frontmatter:{date:"April 20, 2018",components:[["code","C++"],["software","ffmpeg"],["3d","openFrameworks"]],path:"/toolstoolstools",about:"This project is a projected interactive installation. An audio-visual instrument of short animated loops. In homage to the digital toolset of the graphic designer.",cover:"toolstoolstools.png",credits:'Designed with <a target="_blank" href="https://talbaltuch.com">Tal Baltuch</a>',title:"ToolsTools.Tools",press:[["Uncanny – Holon Design Museum","http://www.dmh.org.il/pages/default.aspx?PageId=858"]],links:[["Website","https://toolstools.tools"],["Tal Baltuch","http://talbaltuch.com/ToolsTools-Tools-2"]],embed:"",tags:["Experiment"],excerpt:"An installation about design."}}},pathContext:{prev:{html:"",id:"/home/travis/build/juniorxsound/portfolio/src/pages/projects/soundobjects.md absPath of file >>> MarkdownRemark",frontmatter:{date:"2018-04-21T22:12:03.284Z",path:"/soundobjects",title:"Sound Objects",links:[["Making-of","https://www.youtube.com/watch?v=giU_hnfS8HQ"]],credits:'Developed with <a target="_blank" href="http://www.scottreitherman.com/">Scott Reitherman</a>',press:[],components:[["code","C#, HLSL"],["software","Unity3D, Oculus Rift & Touch SDK"],["3d","Ableton Live, Pure Data, Heavy"]],cover:"so.png",about:"Sound Objects is a music composition app in virtual reality that uses physical objects and collisions to trigger different instruments. The experience challenges our perception of physics by allowing the Sound Objects to bend and defy them, resulting in playful physics that embed musical concepts such as repetition, indeterminism and evolution.",embed:'<iframe width="100%" height="450" src="https://www.youtube.com/embed/EkB9nE-vQhw?rel=0&amp;controls=0&amp;showinfo=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>',excerpt:"Compose music in virtual reality using objects",tags:["Virtual Reality"]}},next:{html:"",id:"/home/travis/build/juniorxsound/portfolio/src/pages/projects/twit.md absPath of file >>> MarkdownRemark",frontmatter:{date:"2018-04-19T22:12:03.284Z",path:"/twit-ar",title:"Twit.AR",links:[["Documentation","http://itp.orfleisher.com/2017/10/21/context-with-twitter-ar/"],["Presskit","http://orfleisher.com/twitter_ar/mediakit.zip"]],credits:'Developed with <a target="_blank" href="http://agermanidis.com">Anastasis Germanidis</a>',press:[["prosthetic knowledge","http://prostheticknowledge.tumblr.com/post/166745203731/twitar-coding-experiment-from-or-fleisher-and"],["Next Reality","https://mobile-ar.reality.news/news/bizarre-ar-experiment-serves-tweets-for-everything-your-iphone-can-see-0180743/"],["Alphr","http://www.alphr.com/twitter/1007491/twitter-in-augmented-reality-looks-like-a-living-nightmare"]],components:[["code","Swift"],["software","Inception v3, CoreML, Twitter API & Swifter"],["3d","ARKit"]],cover:"twit.jpg",about:"TwitAR is a speculative satirical experiment that examines how Twitter tweets could be visualized in Augmented Reality.\nTwitAR tries to playfully imagine what would happen if Twitter intruded our everyday reality. The experiment uses Apple’s ARKit to visualize tweets in Augmented Reality on the world itself.\nTo match the context to what the user is seeing it uses Machine Learning (Apple’s CoreML) to classify the objects you are looking at and pulls tweets from Twitter based on this classification.",embed:'<iframe width="100%" height="450" src="https://www.youtube.com/embed/LVnUHWsGEaQ?rel=0&amp;controls=0&amp;showinfo=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>',excerpt:"Predict how long people have to live in augmented reality.",tags:["Augmented Reality","Machine Learning","Experiment"]}}}}}});
//# sourceMappingURL=path---toolstoolstools-c8c505ef6899314d391f.js.map