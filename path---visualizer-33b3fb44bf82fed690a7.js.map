{"version":3,"sources":["webpack:///path---visualizer-33b3fb44bf82fed690a7.js","webpack:///./.cache/json/visualizer.json"],"names":["webpackJsonp","394","module","exports","data","markdownRemark","html","frontmatter","date","components","path","about","cover","credits","title","press","links","embed","tags","excerpt","pathContext","prev","id","next"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAkBC,MAAQC,gBAAkBC,KAAA,GAAAC,aAAyBC,KAAA,iBAAAC,aAAA,gGAAAC,KAAA,cAAAC,MAAA,0pBAAAC,MAAA,iBAAAC,QAAA,GAAAC,MAAA,aAAAC,SAAAC,QAAA,gLAAAC,MAAA,GAAAC,MAAA,cAAAC,QAAA,sCAA8oCC,aAAgBC,MAAQf,KAAA,GAAAgB,GAAA,4GAAAf,aAA0IC,KAAA,4BAAAE,KAAA,UAAAI,MAAA,SAAAE,QAAA,iNAAAH,QAAA,kFAAAE,QAAA,qbAAAN,aAAA,6IAAAG,MAAA,mBAAAD,MAAA,sgBAAAM,MAAA,GAAAE,QAAA,gEAAAD,MAAA,oEAAoqDK,KAAA","file":"path---visualizer-33b3fb44bf82fed690a7.js","sourcesContent":["webpackJsonp([109302176427146],{\n\n/***/ 394:\n/***/ (function(module, exports) {\n\n\tmodule.exports = {\"data\":{\"markdownRemark\":{\"html\":\"\",\"frontmatter\":{\"date\":\"April 23, 2018\",\"components\":[[\"code\",\"Javascript, GLSL\"],[\"software\",\"Blender, e-on Vue, Adobe Photoshop\"],[\"3d\",\"Three.js\"]],\"path\":\"/visualizer\",\"about\":\"‘Visualizer’ was born after an R&D process I did with Roey Tsemah and Ronen Tanchum. Initially our goal was to research how we could use the Web Audio API, to analyse incoming signal (music or microphone inputs), and drive geometry deformations in real-time, while keeping performance optimal so it could run on mobile browsers as well.\\nAfter the R&D process was done, I went on to design the concept art for this visualizer, using 3D animation and environment design software packages and then implementing these designs in the web using Three.js. This work was made possible thanks to the amazing post “Experiments with Perlin Noise” examples by Jaume Sanchez.\",\"cover\":\"visualizer.png\",\"credits\":\"\",\"title\":\"Visualizer\",\"press\":[],\"links\":[[\"Demo\",\"https://juniorxsound.github.io/ICM-Fall-2016/3D_Web_Audio_Visualiser/\"],[\"Github\",\"https://github.com/juniorxsound/ICM-Fall-2016/tree/master/3D_Web_Audio_Visualiser\"]],\"embed\":\"\",\"tags\":[\"Experiment\"],\"excerpt\":\"A 3D web audio sound visulaizer.\"}}},\"pathContext\":{\"prev\":{\"html\":\"\",\"id\":\"/home/travis/build/juniorxsound/portfolio/src/pages/projects/volume.md absPath of file >>> MarkdownRemark\",\"frontmatter\":{\"date\":\"2018-04-23T12:34:00+00:02\",\"path\":\"/volume\",\"title\":\"Volume\",\"links\":[[\"Website\",\"https://volume.gl\"],[\"Github\",\"https://github.com/Volume-GL\"],[\"Presskit\",\"https://drive.google.com/drive/folders/1XBQgptNAchJr0kUSD0LhzUzxdKnZ4Rud\"],[\"Presentation\",\"https://vimeo.com/270479574\"]],\"credits\":\"Developed with <a target=\\\"_blank\\\" href=\\\"https://shirin.works\\\">~shirin anlen</a>\",\"press\":[[\"The Next Web\",\"https://thenextweb.com/artificial-intelligence/2018/03/08/try-this-ai-experiment-that-converts-2d-images-to-3d/\"],[\"Discovery Channel\",\"https://www.youtube.com/watch?v=Zi4yof2yy04\"],[\"prosthetic knowledge\",\"http://prostheticknowledge.tumblr.com/post/171637247736/volume-online-update-update-to-machine-learning\"],[\"Tecmundo\",\"https://www.tecmundo.com.br/software/127998-ia-transforma-qualquer-foto-modelo-3d-teste.htm\"]],\"components\":[[\"code\",\"Python, Javascript, GLSL, C#, HLSL\"],[\"software\",\"Three.js, Unity3D, Unreal Engine, Blender\"],[\"3d\",\"Tensorflow, Heroku, Firebase\"]],\"cover\":\"volume_cover.jpg\",\"about\":\"Volume is a tool for reconstructing a single 2D image or video in 3D space. Using state-of-the-art machine learning research, Volume is able to generate a 3D asset from a single view. Volume is currently under development and is being built as an end-to-end solution allowing anyone to easily generate a 3D asset and use it in 3D environments. Volume is intended to encourage easy prototyping in virtual, augmented and mixed reality platforms. Volume was used to create the Inside Pulp Fiction project, and ReTouch.\",\"embed\":\"\",\"excerpt\":\"Reconstruct 2D images and video in 3D using machine learning.\",\"tags\":[\"Augmented Reality\",\"Machine Learning\",\"Virtual Reality\",\"Tools\"]}},\"next\":null}}\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// path---visualizer-33b3fb44bf82fed690a7.js","module.exports = {\"data\":{\"markdownRemark\":{\"html\":\"\",\"frontmatter\":{\"date\":\"April 23, 2018\",\"components\":[[\"code\",\"Javascript, GLSL\"],[\"software\",\"Blender, e-on Vue, Adobe Photoshop\"],[\"3d\",\"Three.js\"]],\"path\":\"/visualizer\",\"about\":\"‘Visualizer’ was born after an R&D process I did with Roey Tsemah and Ronen Tanchum. Initially our goal was to research how we could use the Web Audio API, to analyse incoming signal (music or microphone inputs), and drive geometry deformations in real-time, while keeping performance optimal so it could run on mobile browsers as well.\\nAfter the R&D process was done, I went on to design the concept art for this visualizer, using 3D animation and environment design software packages and then implementing these designs in the web using Three.js. This work was made possible thanks to the amazing post “Experiments with Perlin Noise” examples by Jaume Sanchez.\",\"cover\":\"visualizer.png\",\"credits\":\"\",\"title\":\"Visualizer\",\"press\":[],\"links\":[[\"Demo\",\"https://juniorxsound.github.io/ICM-Fall-2016/3D_Web_Audio_Visualiser/\"],[\"Github\",\"https://github.com/juniorxsound/ICM-Fall-2016/tree/master/3D_Web_Audio_Visualiser\"]],\"embed\":\"\",\"tags\":[\"Experiment\"],\"excerpt\":\"A 3D web audio sound visulaizer.\"}}},\"pathContext\":{\"prev\":{\"html\":\"\",\"id\":\"/home/travis/build/juniorxsound/portfolio/src/pages/projects/volume.md absPath of file >>> MarkdownRemark\",\"frontmatter\":{\"date\":\"2018-04-23T12:34:00+00:02\",\"path\":\"/volume\",\"title\":\"Volume\",\"links\":[[\"Website\",\"https://volume.gl\"],[\"Github\",\"https://github.com/Volume-GL\"],[\"Presskit\",\"https://drive.google.com/drive/folders/1XBQgptNAchJr0kUSD0LhzUzxdKnZ4Rud\"],[\"Presentation\",\"https://vimeo.com/270479574\"]],\"credits\":\"Developed with <a target=\\\"_blank\\\" href=\\\"https://shirin.works\\\">~shirin anlen</a>\",\"press\":[[\"The Next Web\",\"https://thenextweb.com/artificial-intelligence/2018/03/08/try-this-ai-experiment-that-converts-2d-images-to-3d/\"],[\"Discovery Channel\",\"https://www.youtube.com/watch?v=Zi4yof2yy04\"],[\"prosthetic knowledge\",\"http://prostheticknowledge.tumblr.com/post/171637247736/volume-online-update-update-to-machine-learning\"],[\"Tecmundo\",\"https://www.tecmundo.com.br/software/127998-ia-transforma-qualquer-foto-modelo-3d-teste.htm\"]],\"components\":[[\"code\",\"Python, Javascript, GLSL, C#, HLSL\"],[\"software\",\"Three.js, Unity3D, Unreal Engine, Blender\"],[\"3d\",\"Tensorflow, Heroku, Firebase\"]],\"cover\":\"volume_cover.jpg\",\"about\":\"Volume is a tool for reconstructing a single 2D image or video in 3D space. Using state-of-the-art machine learning research, Volume is able to generate a 3D asset from a single view. Volume is currently under development and is being built as an end-to-end solution allowing anyone to easily generate a 3D asset and use it in 3D environments. Volume is intended to encourage easy prototyping in virtual, augmented and mixed reality platforms. Volume was used to create the Inside Pulp Fiction project, and ReTouch.\",\"embed\":\"\",\"excerpt\":\"Reconstruct 2D images and video in 3D using machine learning.\",\"tags\":[\"Augmented Reality\",\"Machine Learning\",\"Virtual Reality\",\"Tools\"]}},\"next\":null}}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/json-loader!./.cache/json/visualizer.json\n// module id = 394\n// module chunks = 109302176427146"],"sourceRoot":""}