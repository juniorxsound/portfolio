webpackJsonp([0x9bc4b7dfb2fc],{381:function(e,t){e.exports={data:{markdownRemark:{html:"",frontmatter:{date:"May 23, 2018",components:[["code","Javascript, GLSL"],["software","Blender, e-on Vue, TouchDesigner, Autodesk Maya and Ableton Live, Web Audio API, Web MIDI API & WebVR API."],["3d","Three.js"]],path:"/sono",about:"‘SONO’ is a binaural webVR musical performance featuring music from Livyatanim’s debut album ‘After the Waters’.\nThe ‘venue’ in which the band plays is a dark crater located in a surreal outer-space environment, surrounded by cosmic events and astronomical phenomenons. ‘SONO’ features three songs, each of them played by the band as the surrounding world changes around them. The music, like the visuals – is binaural, allowing the audience to move around and hear what they would hear if they were surrounded by the band.\nThe experience can be watched on a wide range of platforms from desktop computers, mobile phones and VR headsets.",cover:"sono.png",credits:"Developed with Yannis Gravezas, Ronen Tanchum, Ilya Marcus and Livyatanim",title:"Sono",press:[["Creators Project","https://creators.vice.com/en_us/article/aenxpb/sono-livyatanim-audio-reactive-live-vr-performance"],["WebVR Experiments with Google","https://experiments.withgoogle.com/sono"],["VRRoom","https://www.vrroom.buzz/vr-news/immersive-arts/cosmic-visuals-react-live-audio-vr-show"]],links:[["Full Experience","http://sono.livyatanim.com/"],["Album","https://livyatanim.bandcamp.com"],["Presskit","http://sono.livyatanim.com/media/sono_mediakit.zip"],["Making-of","https://www.youtube.com/watch?v=5_0eb7B9yoo"]],embed:'<iframe width="100%" height="450" src="https://www.youtube.com/embed/e30AUS9HFtE?rel=0&amp;controls=1&amp;showinfo=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>',tags:["Virtual Reality"],excerpt:"A cosmic webVR music performance"}}},pathContext:{prev:{html:"",id:"/home/travis/build/juniorxsound/portfolio/src/pages/projects/skeletron.md absPath of file >>> MarkdownRemark",frontmatter:{date:"2018-04-23T12:34:00+00:02",path:"/skeletron",title:"Skeletron",links:[["Presskit","https://drive.google.com/drive/folders/18uzf-grMetd9bZPNMHDNs7IZWZHNFmKY"]],credits:'Developed with <a target="_blank" href="https://drorayalon.com">Dror Ayalon</a>. <br />Attribution: VNect ML Model VNect TensorFlow Port',press:[["The Next Web","https://thenextweb.com/artificial-intelligence/2018/01/30/programmers-use-tensorflow-ai-to-turn-any-webcam-into-microsoft-kinect/"],["Tech Radar","https://www.techradar.com/news/ai-developers-can-turn-any-webcam-into-a-kinect"],["GeekTime","https://www.geektime.co.il/developers-create-kinect-with-tensorflow-and-webcam/"],["Android Headlines","https://www.androidheadlines.com/2018/01/tensorflow-unity-turn-webcams-into-ai-powered-ar-systems.html"],["FossBytes","https://fossbytes.com/programmers-transform-a-10-webcam-into-microsoft-kinect/"]],components:[["code","C#, HLSL Python"],["software","Unity3D"],["3d","Tensorflow"]],cover:"skeletron.jpg",about:"Skeletron is a system that predicts joints and human skeleton position from real-time video taken by any RGB camera, such as a webcam. The system sends the data about the position of the human body to Unity, a 3D game development engine, to allow engineers, artists, and creative technologists to use it to develop digital experiences.\nSkeletron was developed thanks and as a part of NYU ITP Xstory grant (Experiments in Storytelling).",embed:'<iframe width="100%" height="500" src="https://www.youtube.com/embed/l_owi316cE8?rel=0&amp;controls=0&amp;showinfo=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>',excerpt:"Predict joints and human skeleton position from real-time video.",tags:["Machine Learning","Tools"]}},next:{html:"",id:"/home/travis/build/juniorxsound/portfolio/src/pages/projects/soundobjects.md absPath of file >>> MarkdownRemark",frontmatter:{date:"2018-05-23T12:34:00+00:00",path:"/soundobjects",title:"Sound Objects",links:[["Making-of","https://www.youtube.com/watch?v=giU_hnfS8HQ"]],credits:'Developed with <a target="_blank" href="http://www.scottreitherman.com/">Scott Reitherman</a>',press:[],components:[["code","C#, HLSL"],["software","Unity3D, Oculus Rift & Touch SDK"],["3d","Ableton Live, Pure Data, Heavy"]],cover:"so.png",about:"Sound Objects is a music composition app in virtual reality that uses physical objects and collisions to trigger different instruments. The experience challenges our perception of physics by allowing the Sound Objects to bend and defy them, resulting in playful physics that embed musical concepts such as repetition, indeterminism and evolution.",embed:'<iframe width="100%" height="450" src="https://www.youtube.com/embed/EkB9nE-vQhw?rel=0&amp;controls=0&amp;showinfo=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>',excerpt:"Compose music in virtual reality using objects",tags:["Virtual Reality"]}}}}}});
//# sourceMappingURL=path---sono-66121137388b2e4f09f1.js.map