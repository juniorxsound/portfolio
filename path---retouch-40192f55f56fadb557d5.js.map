{"version":3,"sources":["webpack:///path---retouch-40192f55f56fadb557d5.js","webpack:///./.cache/json/retouch.json"],"names":["webpackJsonp","383","module","exports","data","markdownRemark","html","frontmatter","date","components","path","about","cover","credits","title","press","links","embed","tags","excerpt","pathContext","prev","id","next"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAkBC,MAAQC,gBAAkBC,KAAA,GAAAC,aAAyBC,KAAA,iBAAAC,aAAA,2DAAAC,KAAA,WAAAC,MAAA,4YAAAC,MAAA,cAAAC,QAAA,qIAAAC,MAAA,UAAAC,SAAAC,QAAA,qDAAAC,MAAA,oMAAi7BC,MAAA,4BAAAC,QAAA,yCAAoIC,aAAgBC,MAAQf,KAAA,GAAAgB,GAAA,0GAAAf,aAAwIC,KAAA,4BAAAE,KAAA,QAAAI,MAAA,OAAAE,QAAA,0JAAAH,QAAA,8DAAAE,QAAA,+xBAAAN,aAAA,0KAAAG,MAAA,iBAAAD,MAAA,ipBAAAM,MAAA,2WAA+pEE,QAAA,gDAAAD,MAAA,qBAAqPK,MAASjB,KAAA,GAAAgB,GAAA,+GAAAf,aAA6IC,KAAA,4BAAAE,KAAA,aAAAI,MAAA,YAAAE,QAAA,wFAAAH,QAAA,2IAAAE,QAAA,kjBAAAN,aAAA,sEAAAG,MAAA,gBAAAD,MAAA,ubAAAM,MAAA,oMAAslDE,QAAA,mEAAAD,MAAA","file":"path---retouch-40192f55f56fadb557d5.js","sourcesContent":["webpackJsonp([262858279625854],{\n\n/***/ 383:\n/***/ (function(module, exports) {\n\n\tmodule.exports = {\"data\":{\"markdownRemark\":{\"html\":\"\",\"frontmatter\":{\"date\":\"April 23, 2018\",\"components\":[[\"code\",\"C++, GLSL\"],[\"software\",\"Volume\"],[\"3d\",\"OpenGL\"]],\"path\":\"/retouch\",\"about\":\"ReTouch is an OpenGL application that enables editing and retouching of images using depth-maps in 2.5D. The depth maps are generated by Volume, a state of the art tool, that uses a CNN (Convolutional Neural Network) to predict depth-maps from 2D images . ReTouch uses these depth-maps to enable the addition of depth of field and color retouching for the foreground and background separately.\",\"cover\":\"shining.jpg\",\"credits\":\"Developed by under the advisement of Prof. Ken Perlin and Prof. Daniele Panozzo @ Computer Science Department, New York University\",\"title\":\"ReTouch\",\"press\":[],\"links\":[[\"Github\",\"https://github.com/juniorxsound/ReTouch\"]],\"embed\":\"<iframe width=\\\"100%\\\" height=\\\"450\\\" src=\\\"https://www.youtube.com/embed/CAsy_jm85ZY?rel=0&amp;controls=0&amp;showinfo=0\\\" frameborder=\\\"0\\\" allow=\\\"autoplay; encrypted-media\\\" allowfullscreen></iframe>\",\"tags\":[\"Machine Learning\",\"Tools\"],\"excerpt\":\"Edit and retouch any image in 2.5D.\"}}},\"pathContext\":{\"prev\":{\"html\":\"\",\"id\":\"/home/travis/build/juniorxsound/portfolio/src/pages/projects/myth.md absPath of file >>> MarkdownRemark\",\"frontmatter\":{\"date\":\"2018-05-23T12:34:00+00:00\",\"path\":\"/myth\",\"title\":\"Myth\",\"links\":[[\"Full Experience\",\"http://film.livyatanim.com\"],[\"Album\",\"https://livyatanim.bandcamp.com\"],[\"Presskit\",\"http://film.livyatanim.com/media/mediakit.zip\"]],\"credits\":\"Developed with Yannis Gravezas, Tomer Rousso and Livyatanim\",\"press\":[[\"Wired\",\"https://www.wired.de/collection/life/10-virtual-reality-filme-die-man-gesehen-haben-muss\"],[\"Creators Project\",\"https://creators.vice.com/en_us/article/ez5qva/float-through-a-virtual-world-of-hybrid-beings-in-myth\"],[\"We and the Color\",\"https://weandthecolor.com/webgl-short-film-livyatanim-myth/62302\"],[\"prosthetic knowledge\",\"http://prostheticknowledge.tumblr.com/post/133824524661/myth-interactive-web-music-video-for-livyatanim'\"],[\"Z\",\"http://z.ultranoir.com/en/articles/1282-livyatanim-myth-a-vr-film-by-or-fleisher.html\"],[\"Chrome Experiments\",\"https://experiments.withgoogle.com/livyatanim-myth\"],[\"WorldFest- NASA Remi Award winner\",\"#\"],[\"UrbamMediaMakers Best Interactive Award Winner\",\"#\"],[\"The FWA – WOTD\",\"#\"],[\"CSS Awards – WOTD\",\"#\"],[\"Awwwards – Honorable Mention\",\"#\"]],\"components\":[[\"code\",\"Javascript, GLSL\"],[\"software\",\"Blender, e-on Vue, Adobe Photoshop, Autodesk Maya and Ableton Live, Web Audio API, Web MIDI API & WebVR API.\"],[\"3d\",\"Three.js\"]],\"cover\":\"myth-cover.png\",\"about\":\"‘Myth’, is an interactive web virtual reality short film, featuring the song “Can I peacfuly Love” from Livyatanim’s debut album “After the Waters”. The film takes place in a dark surreal world, which aims to blur the lines between digital and natural imagery.\\nThe film uses the composition’s notation, rhythms and melodies (MIDI), to control elements ranging from drums affecting the geometry to transitions between scenes. In effect, using this data transformed from being a musical composition language, to a visual directing language.\\nThe experience can be watched on a wide range of platforms from desktop computers, mobile phones and VR headsets.\",\"embed\":\"<div style=\\\"padding:56.25% 0 0 0;position:relative;\\\"><iframe src=\\\"https://player.vimeo.com/video/145578640?autoplay=0&title=0&byline=0&portrait=0\\\" style=\\\"position:absolute;top:0;left:0;width:100%;height:100%;\\\" frameborder=\\\"0\\\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div><script src=\\\"https://player.vimeo.com/api/player.js\\\"></script>\",\"excerpt\":\"An audio reactive virtual reality short film.\",\"tags\":[\"Virtual Reality\"]}},\"next\":{\"html\":\"\",\"id\":\"/home/travis/build/juniorxsound/portfolio/src/pages/projects/skeletron.md absPath of file >>> MarkdownRemark\",\"frontmatter\":{\"date\":\"2018-04-23T12:34:00+00:02\",\"path\":\"/skeletron\",\"title\":\"Skeletron\",\"links\":[[\"Presskit\",\"https://drive.google.com/drive/folders/18uzf-grMetd9bZPNMHDNs7IZWZHNFmKY\"]],\"credits\":\"Developed with <a target=\\\"_blank\\\" href=\\\"https://drorayalon.com\\\">Dror Ayalon</a>. <br />Attribution: VNect ML Model VNect TensorFlow Port\",\"press\":[[\"The Next Web\",\"https://thenextweb.com/artificial-intelligence/2018/01/30/programmers-use-tensorflow-ai-to-turn-any-webcam-into-microsoft-kinect/\"],[\"Tech Radar\",\"https://www.techradar.com/news/ai-developers-can-turn-any-webcam-into-a-kinect\"],[\"GeekTime\",\"https://www.geektime.co.il/developers-create-kinect-with-tensorflow-and-webcam/\"],[\"Android Headlines\",\"https://www.androidheadlines.com/2018/01/tensorflow-unity-turn-webcams-into-ai-powered-ar-systems.html\"],[\"FossBytes\",\"https://fossbytes.com/programmers-transform-a-10-webcam-into-microsoft-kinect/\"]],\"components\":[[\"code\",\"C#, HLSL Python\"],[\"software\",\"Unity3D\"],[\"3d\",\"Tensorflow\"]],\"cover\":\"skeletron.jpg\",\"about\":\"Skeletron is a system that predicts joints and human skeleton position from real-time video taken by any RGB camera, such as a webcam. The system sends the data about the position of the human body to Unity, a 3D game development engine, to allow engineers, artists, and creative technologists to use it to develop digital experiences.\\nSkeletron was developed thanks and as a part of NYU ITP Xstory grant (Experiments in Storytelling).\",\"embed\":\"<iframe width=\\\"100%\\\" height=\\\"500\\\" src=\\\"https://www.youtube.com/embed/l_owi316cE8?rel=0&amp;controls=0&amp;showinfo=0\\\" frameborder=\\\"0\\\" allow=\\\"autoplay; encrypted-media\\\" allowfullscreen></iframe>\",\"excerpt\":\"Predict joints and human skeleton position from real-time video.\",\"tags\":[\"Machine Learning\",\"Tools\"]}}}}\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// path---retouch-40192f55f56fadb557d5.js","module.exports = {\"data\":{\"markdownRemark\":{\"html\":\"\",\"frontmatter\":{\"date\":\"April 23, 2018\",\"components\":[[\"code\",\"C++, GLSL\"],[\"software\",\"Volume\"],[\"3d\",\"OpenGL\"]],\"path\":\"/retouch\",\"about\":\"ReTouch is an OpenGL application that enables editing and retouching of images using depth-maps in 2.5D. The depth maps are generated by Volume, a state of the art tool, that uses a CNN (Convolutional Neural Network) to predict depth-maps from 2D images . ReTouch uses these depth-maps to enable the addition of depth of field and color retouching for the foreground and background separately.\",\"cover\":\"shining.jpg\",\"credits\":\"Developed by under the advisement of Prof. Ken Perlin and Prof. Daniele Panozzo @ Computer Science Department, New York University\",\"title\":\"ReTouch\",\"press\":[],\"links\":[[\"Github\",\"https://github.com/juniorxsound/ReTouch\"]],\"embed\":\"<iframe width=\\\"100%\\\" height=\\\"450\\\" src=\\\"https://www.youtube.com/embed/CAsy_jm85ZY?rel=0&amp;controls=0&amp;showinfo=0\\\" frameborder=\\\"0\\\" allow=\\\"autoplay; encrypted-media\\\" allowfullscreen></iframe>\",\"tags\":[\"Machine Learning\",\"Tools\"],\"excerpt\":\"Edit and retouch any image in 2.5D.\"}}},\"pathContext\":{\"prev\":{\"html\":\"\",\"id\":\"/home/travis/build/juniorxsound/portfolio/src/pages/projects/myth.md absPath of file >>> MarkdownRemark\",\"frontmatter\":{\"date\":\"2018-05-23T12:34:00+00:00\",\"path\":\"/myth\",\"title\":\"Myth\",\"links\":[[\"Full Experience\",\"http://film.livyatanim.com\"],[\"Album\",\"https://livyatanim.bandcamp.com\"],[\"Presskit\",\"http://film.livyatanim.com/media/mediakit.zip\"]],\"credits\":\"Developed with Yannis Gravezas, Tomer Rousso and Livyatanim\",\"press\":[[\"Wired\",\"https://www.wired.de/collection/life/10-virtual-reality-filme-die-man-gesehen-haben-muss\"],[\"Creators Project\",\"https://creators.vice.com/en_us/article/ez5qva/float-through-a-virtual-world-of-hybrid-beings-in-myth\"],[\"We and the Color\",\"https://weandthecolor.com/webgl-short-film-livyatanim-myth/62302\"],[\"prosthetic knowledge\",\"http://prostheticknowledge.tumblr.com/post/133824524661/myth-interactive-web-music-video-for-livyatanim'\"],[\"Z\",\"http://z.ultranoir.com/en/articles/1282-livyatanim-myth-a-vr-film-by-or-fleisher.html\"],[\"Chrome Experiments\",\"https://experiments.withgoogle.com/livyatanim-myth\"],[\"WorldFest- NASA Remi Award winner\",\"#\"],[\"UrbamMediaMakers Best Interactive Award Winner\",\"#\"],[\"The FWA – WOTD\",\"#\"],[\"CSS Awards – WOTD\",\"#\"],[\"Awwwards – Honorable Mention\",\"#\"]],\"components\":[[\"code\",\"Javascript, GLSL\"],[\"software\",\"Blender, e-on Vue, Adobe Photoshop, Autodesk Maya and Ableton Live, Web Audio API, Web MIDI API & WebVR API.\"],[\"3d\",\"Three.js\"]],\"cover\":\"myth-cover.png\",\"about\":\"‘Myth’, is an interactive web virtual reality short film, featuring the song “Can I peacfuly Love” from Livyatanim’s debut album “After the Waters”. The film takes place in a dark surreal world, which aims to blur the lines between digital and natural imagery.\\nThe film uses the composition’s notation, rhythms and melodies (MIDI), to control elements ranging from drums affecting the geometry to transitions between scenes. In effect, using this data transformed from being a musical composition language, to a visual directing language.\\nThe experience can be watched on a wide range of platforms from desktop computers, mobile phones and VR headsets.\",\"embed\":\"<div style=\\\"padding:56.25% 0 0 0;position:relative;\\\"><iframe src=\\\"https://player.vimeo.com/video/145578640?autoplay=0&title=0&byline=0&portrait=0\\\" style=\\\"position:absolute;top:0;left:0;width:100%;height:100%;\\\" frameborder=\\\"0\\\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div><script src=\\\"https://player.vimeo.com/api/player.js\\\"></script>\",\"excerpt\":\"An audio reactive virtual reality short film.\",\"tags\":[\"Virtual Reality\"]}},\"next\":{\"html\":\"\",\"id\":\"/home/travis/build/juniorxsound/portfolio/src/pages/projects/skeletron.md absPath of file >>> MarkdownRemark\",\"frontmatter\":{\"date\":\"2018-04-23T12:34:00+00:02\",\"path\":\"/skeletron\",\"title\":\"Skeletron\",\"links\":[[\"Presskit\",\"https://drive.google.com/drive/folders/18uzf-grMetd9bZPNMHDNs7IZWZHNFmKY\"]],\"credits\":\"Developed with <a target=\\\"_blank\\\" href=\\\"https://drorayalon.com\\\">Dror Ayalon</a>. <br />Attribution: VNect ML Model VNect TensorFlow Port\",\"press\":[[\"The Next Web\",\"https://thenextweb.com/artificial-intelligence/2018/01/30/programmers-use-tensorflow-ai-to-turn-any-webcam-into-microsoft-kinect/\"],[\"Tech Radar\",\"https://www.techradar.com/news/ai-developers-can-turn-any-webcam-into-a-kinect\"],[\"GeekTime\",\"https://www.geektime.co.il/developers-create-kinect-with-tensorflow-and-webcam/\"],[\"Android Headlines\",\"https://www.androidheadlines.com/2018/01/tensorflow-unity-turn-webcams-into-ai-powered-ar-systems.html\"],[\"FossBytes\",\"https://fossbytes.com/programmers-transform-a-10-webcam-into-microsoft-kinect/\"]],\"components\":[[\"code\",\"C#, HLSL Python\"],[\"software\",\"Unity3D\"],[\"3d\",\"Tensorflow\"]],\"cover\":\"skeletron.jpg\",\"about\":\"Skeletron is a system that predicts joints and human skeleton position from real-time video taken by any RGB camera, such as a webcam. The system sends the data about the position of the human body to Unity, a 3D game development engine, to allow engineers, artists, and creative technologists to use it to develop digital experiences.\\nSkeletron was developed thanks and as a part of NYU ITP Xstory grant (Experiments in Storytelling).\",\"embed\":\"<iframe width=\\\"100%\\\" height=\\\"500\\\" src=\\\"https://www.youtube.com/embed/l_owi316cE8?rel=0&amp;controls=0&amp;showinfo=0\\\" frameborder=\\\"0\\\" allow=\\\"autoplay; encrypted-media\\\" allowfullscreen></iframe>\",\"excerpt\":\"Predict joints and human skeleton position from real-time video.\",\"tags\":[\"Machine Learning\",\"Tools\"]}}}}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/json-loader!./.cache/json/retouch.json\n// module id = 383\n// module chunks = 262858279625854"],"sourceRoot":""}