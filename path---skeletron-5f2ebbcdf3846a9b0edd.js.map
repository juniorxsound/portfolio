{"version":3,"sources":["webpack:///path---skeletron-5f2ebbcdf3846a9b0edd.js","webpack:///./.cache/json/skeletron.json"],"names":["webpackJsonp","384","module","exports","data","markdownRemark","html","frontmatter","date","components","path","about","cover","credits","title","press","links","embed","tags","excerpt","pathContext","prev","id","next"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAkBC,MAAQC,gBAAkBC,KAAA,GAAAC,aAAyBC,KAAA,iBAAAC,aAAA,sEAAAC,KAAA,aAAAC,MAAA,ubAAAC,MAAA,gBAAAC,QAAA,2IAAAC,MAAA,YAAAC,QAAA,kjBAAAC,QAAA,wFAAAC,MAAA,oMAA2kDC,MAAA,4BAAAC,QAAA,sEAAiKC,aAAgBC,MAAQf,KAAA,GAAAgB,GAAA,6GAAAf,aAA2IC,KAAA,4BAAAE,KAAA,WAAAI,MAAA,UAAAE,QAAA,qDAAAH,QAAA,qIAAAE,SAAAN,aAAA,2DAAAG,MAAA,cAAAD,MAAA,4YAAAM,MAAA,oMAA47BE,QAAA,sCAAAD,MAAA,8BAAmIK,MAASjB,KAAA,GAAAgB,GAAA,0GAAAf,aAAwIC,KAAA,4BAAAE,KAAA,QAAAI,MAAA,OAAAE,QAAA,4NAAAH,QAAA,4EAAAE,QAAA,ySAAAN,aAAA,wKAAAG,MAAA,WAAAD,MAAA,koBAAAM,MAAA,oMAA4qDE,QAAA,mCAAAD,MAAA","file":"path---skeletron-5f2ebbcdf3846a9b0edd.js","sourcesContent":["webpackJsonp([101821273434762],{\n\n/***/ 384:\n/***/ (function(module, exports) {\n\n\tmodule.exports = {\"data\":{\"markdownRemark\":{\"html\":\"\",\"frontmatter\":{\"date\":\"April 23, 2018\",\"components\":[[\"code\",\"C#, HLSL Python\"],[\"software\",\"Unity3D\"],[\"3d\",\"Tensorflow\"]],\"path\":\"/skeletron\",\"about\":\"Skeletron is a system that predicts joints and human skeleton position from real-time video taken by any RGB camera, such as a webcam. The system sends the data about the position of the human body to Unity, a 3D game development engine, to allow engineers, artists, and creative technologists to use it to develop digital experiences.\\nSkeletron was developed thanks and as a part of NYU ITP Xstory grant (Experiments in Storytelling).\",\"cover\":\"skeletron.jpg\",\"credits\":\"Developed with <a target=\\\"_blank\\\" href=\\\"https://drorayalon.com\\\">Dror Ayalon</a>. <br />Attribution: VNect ML Model VNect TensorFlow Port\",\"title\":\"Skeletron\",\"press\":[[\"The Next Web\",\"https://thenextweb.com/artificial-intelligence/2018/01/30/programmers-use-tensorflow-ai-to-turn-any-webcam-into-microsoft-kinect/\"],[\"Tech Radar\",\"https://www.techradar.com/news/ai-developers-can-turn-any-webcam-into-a-kinect\"],[\"GeekTime\",\"https://www.geektime.co.il/developers-create-kinect-with-tensorflow-and-webcam/\"],[\"Android Headlines\",\"https://www.androidheadlines.com/2018/01/tensorflow-unity-turn-webcams-into-ai-powered-ar-systems.html\"],[\"FossBytes\",\"https://fossbytes.com/programmers-transform-a-10-webcam-into-microsoft-kinect/\"]],\"links\":[[\"Presskit\",\"https://drive.google.com/drive/folders/18uzf-grMetd9bZPNMHDNs7IZWZHNFmKY\"]],\"embed\":\"<iframe width=\\\"100%\\\" height=\\\"500\\\" src=\\\"https://www.youtube.com/embed/l_owi316cE8?rel=0&amp;controls=0&amp;showinfo=0\\\" frameborder=\\\"0\\\" allow=\\\"autoplay; encrypted-media\\\" allowfullscreen></iframe>\",\"tags\":[\"Machine Learning\",\"Tools\"],\"excerpt\":\"Predict joints and human skeleton position from real-time video.\"}}},\"pathContext\":{\"prev\":{\"html\":\"\",\"id\":\"/home/travis/build/juniorxsound/portfolio/src/pages/projects/retouch.md absPath of file >>> MarkdownRemark\",\"frontmatter\":{\"date\":\"2018-04-23T12:34:00+00:02\",\"path\":\"/retouch\",\"title\":\"ReTouch\",\"links\":[[\"Github\",\"https://github.com/juniorxsound/ReTouch\"]],\"credits\":\"Developed by under the advisement of Prof. Ken Perlin and Prof. Daniele Panozzo @ Computer Science Department, New York University\",\"press\":[],\"components\":[[\"code\",\"C++, GLSL\"],[\"software\",\"Volume\"],[\"3d\",\"OpenGL\"]],\"cover\":\"shining.jpg\",\"about\":\"ReTouch is an OpenGL application that enables editing and retouching of images using depth-maps in 2.5D. The depth maps are generated by Volume, a state of the art tool, that uses a CNN (Convolutional Neural Network) to predict depth-maps from 2D images . ReTouch uses these depth-maps to enable the addition of depth of field and color retouching for the foreground and background separately.\",\"embed\":\"<iframe width=\\\"100%\\\" height=\\\"450\\\" src=\\\"https://www.youtube.com/embed/CAsy_jm85ZY?rel=0&amp;controls=0&amp;showinfo=0\\\" frameborder=\\\"0\\\" allow=\\\"autoplay; encrypted-media\\\" allowfullscreen></iframe>\",\"excerpt\":\"Edit and retouch any image in 2.5D.\",\"tags\":[\"Machine Learning\",\"Tools\"]}},\"next\":{\"html\":\"\",\"id\":\"/home/travis/build/juniorxsound/portfolio/src/pages/projects/sono.md absPath of file >>> MarkdownRemark\",\"frontmatter\":{\"date\":\"2018-05-23T12:34:00+00:00\",\"path\":\"/sono\",\"title\":\"Sono\",\"links\":[[\"Full Experience\",\"http://sono.livyatanim.com/\"],[\"Album\",\"https://livyatanim.bandcamp.com\"],[\"Presskit\",\"http://sono.livyatanim.com/media/sono_mediakit.zip\"],[\"Making-of\",\"https://www.youtube.com/watch?v=5_0eb7B9yoo\"]],\"credits\":\"Developed with Yannis Gravezas, Ronen Tanchum, Ilya Marcus and Livyatanim\",\"press\":[[\"Creators Project\",\"https://creators.vice.com/en_us/article/aenxpb/sono-livyatanim-audio-reactive-live-vr-performance\"],[\"WebVR Experiments with Google\",\"https://experiments.withgoogle.com/sono\"],[\"VRRoom\",\"https://www.vrroom.buzz/vr-news/immersive-arts/cosmic-visuals-react-live-audio-vr-show\"]],\"components\":[[\"code\",\"Javascript, GLSL\"],[\"software\",\"Blender, e-on Vue, TouchDesigner, Autodesk Maya and Ableton Live, Web Audio API, Web MIDI API & WebVR API.\"],[\"3d\",\"Three.js\"]],\"cover\":\"sono.png\",\"about\":\"‘SONO’ is a binaural webVR musical performance featuring music from Livyatanim’s debut album ‘After the Waters’.\\nThe ‘venue’ in which the band plays is a dark crater located in a surreal outer-space environment, surrounded by cosmic events and astronomical phenomenons. ‘SONO’ features three songs, each of them played by the band as the surrounding world changes around them. The music, like the visuals – is binaural, allowing the audience to move around and hear what they would hear if they were surrounded by the band.\\nThe experience can be watched on a wide range of platforms from desktop computers, mobile phones and VR headsets.\",\"embed\":\"<iframe width=\\\"100%\\\" height=\\\"450\\\" src=\\\"https://www.youtube.com/embed/e30AUS9HFtE?rel=0&amp;controls=1&amp;showinfo=0\\\" frameborder=\\\"0\\\" allow=\\\"autoplay; encrypted-media\\\" allowfullscreen></iframe>\",\"excerpt\":\"A cosmic webVR music performance\",\"tags\":[\"Virtual Reality\"]}}}}\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// path---skeletron-5f2ebbcdf3846a9b0edd.js","module.exports = {\"data\":{\"markdownRemark\":{\"html\":\"\",\"frontmatter\":{\"date\":\"April 23, 2018\",\"components\":[[\"code\",\"C#, HLSL Python\"],[\"software\",\"Unity3D\"],[\"3d\",\"Tensorflow\"]],\"path\":\"/skeletron\",\"about\":\"Skeletron is a system that predicts joints and human skeleton position from real-time video taken by any RGB camera, such as a webcam. The system sends the data about the position of the human body to Unity, a 3D game development engine, to allow engineers, artists, and creative technologists to use it to develop digital experiences.\\nSkeletron was developed thanks and as a part of NYU ITP Xstory grant (Experiments in Storytelling).\",\"cover\":\"skeletron.jpg\",\"credits\":\"Developed with <a target=\\\"_blank\\\" href=\\\"https://drorayalon.com\\\">Dror Ayalon</a>. <br />Attribution: VNect ML Model VNect TensorFlow Port\",\"title\":\"Skeletron\",\"press\":[[\"The Next Web\",\"https://thenextweb.com/artificial-intelligence/2018/01/30/programmers-use-tensorflow-ai-to-turn-any-webcam-into-microsoft-kinect/\"],[\"Tech Radar\",\"https://www.techradar.com/news/ai-developers-can-turn-any-webcam-into-a-kinect\"],[\"GeekTime\",\"https://www.geektime.co.il/developers-create-kinect-with-tensorflow-and-webcam/\"],[\"Android Headlines\",\"https://www.androidheadlines.com/2018/01/tensorflow-unity-turn-webcams-into-ai-powered-ar-systems.html\"],[\"FossBytes\",\"https://fossbytes.com/programmers-transform-a-10-webcam-into-microsoft-kinect/\"]],\"links\":[[\"Presskit\",\"https://drive.google.com/drive/folders/18uzf-grMetd9bZPNMHDNs7IZWZHNFmKY\"]],\"embed\":\"<iframe width=\\\"100%\\\" height=\\\"500\\\" src=\\\"https://www.youtube.com/embed/l_owi316cE8?rel=0&amp;controls=0&amp;showinfo=0\\\" frameborder=\\\"0\\\" allow=\\\"autoplay; encrypted-media\\\" allowfullscreen></iframe>\",\"tags\":[\"Machine Learning\",\"Tools\"],\"excerpt\":\"Predict joints and human skeleton position from real-time video.\"}}},\"pathContext\":{\"prev\":{\"html\":\"\",\"id\":\"/home/travis/build/juniorxsound/portfolio/src/pages/projects/retouch.md absPath of file >>> MarkdownRemark\",\"frontmatter\":{\"date\":\"2018-04-23T12:34:00+00:02\",\"path\":\"/retouch\",\"title\":\"ReTouch\",\"links\":[[\"Github\",\"https://github.com/juniorxsound/ReTouch\"]],\"credits\":\"Developed by under the advisement of Prof. Ken Perlin and Prof. Daniele Panozzo @ Computer Science Department, New York University\",\"press\":[],\"components\":[[\"code\",\"C++, GLSL\"],[\"software\",\"Volume\"],[\"3d\",\"OpenGL\"]],\"cover\":\"shining.jpg\",\"about\":\"ReTouch is an OpenGL application that enables editing and retouching of images using depth-maps in 2.5D. The depth maps are generated by Volume, a state of the art tool, that uses a CNN (Convolutional Neural Network) to predict depth-maps from 2D images . ReTouch uses these depth-maps to enable the addition of depth of field and color retouching for the foreground and background separately.\",\"embed\":\"<iframe width=\\\"100%\\\" height=\\\"450\\\" src=\\\"https://www.youtube.com/embed/CAsy_jm85ZY?rel=0&amp;controls=0&amp;showinfo=0\\\" frameborder=\\\"0\\\" allow=\\\"autoplay; encrypted-media\\\" allowfullscreen></iframe>\",\"excerpt\":\"Edit and retouch any image in 2.5D.\",\"tags\":[\"Machine Learning\",\"Tools\"]}},\"next\":{\"html\":\"\",\"id\":\"/home/travis/build/juniorxsound/portfolio/src/pages/projects/sono.md absPath of file >>> MarkdownRemark\",\"frontmatter\":{\"date\":\"2018-05-23T12:34:00+00:00\",\"path\":\"/sono\",\"title\":\"Sono\",\"links\":[[\"Full Experience\",\"http://sono.livyatanim.com/\"],[\"Album\",\"https://livyatanim.bandcamp.com\"],[\"Presskit\",\"http://sono.livyatanim.com/media/sono_mediakit.zip\"],[\"Making-of\",\"https://www.youtube.com/watch?v=5_0eb7B9yoo\"]],\"credits\":\"Developed with Yannis Gravezas, Ronen Tanchum, Ilya Marcus and Livyatanim\",\"press\":[[\"Creators Project\",\"https://creators.vice.com/en_us/article/aenxpb/sono-livyatanim-audio-reactive-live-vr-performance\"],[\"WebVR Experiments with Google\",\"https://experiments.withgoogle.com/sono\"],[\"VRRoom\",\"https://www.vrroom.buzz/vr-news/immersive-arts/cosmic-visuals-react-live-audio-vr-show\"]],\"components\":[[\"code\",\"Javascript, GLSL\"],[\"software\",\"Blender, e-on Vue, TouchDesigner, Autodesk Maya and Ableton Live, Web Audio API, Web MIDI API & WebVR API.\"],[\"3d\",\"Three.js\"]],\"cover\":\"sono.png\",\"about\":\"‘SONO’ is a binaural webVR musical performance featuring music from Livyatanim’s debut album ‘After the Waters’.\\nThe ‘venue’ in which the band plays is a dark crater located in a surreal outer-space environment, surrounded by cosmic events and astronomical phenomenons. ‘SONO’ features three songs, each of them played by the band as the surrounding world changes around them. The music, like the visuals – is binaural, allowing the audience to move around and hear what they would hear if they were surrounded by the band.\\nThe experience can be watched on a wide range of platforms from desktop computers, mobile phones and VR headsets.\",\"embed\":\"<iframe width=\\\"100%\\\" height=\\\"450\\\" src=\\\"https://www.youtube.com/embed/e30AUS9HFtE?rel=0&amp;controls=1&amp;showinfo=0\\\" frameborder=\\\"0\\\" allow=\\\"autoplay; encrypted-media\\\" allowfullscreen></iframe>\",\"excerpt\":\"A cosmic webVR music performance\",\"tags\":[\"Virtual Reality\"]}}}}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/json-loader!./.cache/json/skeletron.json\n// module id = 384\n// module chunks = 101821273434762"],"sourceRoot":""}