webpackJsonp([0x9bc4b7dfb2fc],{390:function(e,t){e.exports={data:{markdownRemark:{html:"",frontmatter:{date:"April 26, 2018",components:[["code","Javascript, GLSL"],["software","Blender, e-on Vue, TouchDesigner, Autodesk Maya and Ableton Live, Web Audio API, Web MIDI API & WebVR API."],["3d","Three.js"]],path:"/sono",about:"‘SONO’ is a binaural webVR musical performance featuring music from Livyatanim’s debut album ‘After the Waters’.\nThe ‘venue’ in which the band plays is a dark crater located in a surreal outer-space environment, surrounded by cosmic events and astronomical phenomenons. ‘SONO’ features three songs, each of them played by the band as the surrounding world changes around them. The music, like the visuals – is binaural, allowing the audience to move around and hear what they would hear if they were surrounded by the band.\nThe experience can be watched on a wide range of platforms from desktop computers, mobile phones and VR headsets.",cover:"sono.png",credits:"Developed with Yannis Gravezas, Ronen Tanchum, Ilya Marcus and Livyatanim",title:"Sono",press:[["Creators Project","https://creators.vice.com/en_us/article/aenxpb/sono-livyatanim-audio-reactive-live-vr-performance"],["WebVR Experiments with Google","https://experiments.withgoogle.com/sono"],["VRRoom","https://www.vrroom.buzz/vr-news/immersive-arts/cosmic-visuals-react-live-audio-vr-show"]],links:[["Full Experience","http://sono.livyatanim.com/"],["Album","https://livyatanim.bandcamp.com"],["Presskit","http://sono.livyatanim.com/media/sono_mediakit.zip"],["Making-of","https://www.youtube.com/watch?v=5_0eb7B9yoo"]],embed:'<iframe width="100%" height="450" src="https://www.youtube.com/embed/e30AUS9HFtE?rel=0&amp;controls=1&amp;showinfo=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>',tags:["Virtual Reality"],excerpt:"A cosmic webVR music performance"}}},pathContext:{prev:{html:"",id:"/home/travis/build/juniorxsound/portfolio/src/pages/projects/pulpfiction.md absPath of file >>> MarkdownRemark",frontmatter:{date:"2018-04-27T22:12:03.284Z",path:"/pulp-fiction",title:"Inside Pulp Fiction",links:[["Website","https://volume.gl"],["Github","https://github.com/Volume-GL/Pulp-Fiction-ARKit"],["Presskit","https://drive.google.com/drive/folders/1XBQgptNAchJr0kUSD0LhzUzxdKnZ4Rud"],["Presentation","https://vimeo.com/270479574"]],credits:'Developed with <a target="_blank" href="https://shirin.works">~shirin anlen</a>',press:[["The Next Web","https://thenextweb.com/artificial-intelligence/2018/01/22/ai-rips-objects-from-video-and-reimagines-them-in-3d-ar/"],["Discovery Channel","https://www.youtube.com/watch?v=Zi4yof2yy04"],["Vice","https://motherboard.vice.com/en_us/article/gywamy/cue-up-the-pulp-fiction-dance-scene-this-app-3d-projects-2d-movies-in-your-living-room"],["Mashable","http://mashable.france24.com/tech-business/20180130-films-volume-realite-augmentee-cinema-technologie"],["UploadVR","https://uploadvr.com/ar-app-brings-pulp-fiction-characters-living-room/"],["prosthetic knowledge","http://prostheticknowledge.tumblr.com/post/170014746561/volume-in-development-project-from-or-fleisher"],["VRFocus","https://www.vrfocus.com/2018/01/reconstruct-your-favourite-movie-in-ar/"],["Android Headlines","https://www.androidheadlines.com/2018/01/volume-ai-program-puts-2d-objects-3d-spaces.html"],["Labroots","https://www.labroots.com/trending/videos/11371/ai-tool-turns-video-into-3d-augmented-reality-experiences"]],components:[["code","Python, C#, HLSL"],["software","Unity3D, Tensorflow"],["3d","ARKit, Volume"]],cover:"pulp_cover.jpg",about:"Inside Pulp Fiction is an experiment that uses machine learning to reconstruct Pulp Fiction's iconic dance scene in Augmented Reality. The experiment is a part of Volume, a machine learning driven tool to reconstruct 3D models from 2D images and video.",embed:'<iframe width="100%" height="450" src="https://www.youtube.com/embed/iwJt4DM6mJA?rel=0&amp;controls=0&amp;showinfo=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>',excerpt:"Step inside Pulp Fiction's iconic dance scene using augmented reality.",tags:["Augmented Reality","Machine Learning"]}},next:{html:"",id:"/home/travis/build/juniorxsound/portfolio/src/pages/projects/depthkitjs.md absPath of file >>> MarkdownRemark",frontmatter:{date:"2018-04-25T22:12:03.284Z",path:"/depthkit-js",title:"DepthKit.js",links:[["Github","https://github.com/juniorxsound/DepthKit.js"],["Documentation","https://juniorxsound.github.io/DepthKit.js/"],["npm package","https://www.npmjs.com/package/depthkit"]],credits:"",press:[],components:[["code","Javascript, GLSL"],["software","DepthKit"],["3d","Three.js"]],cover:"depthkitjs_cover.png",about:"DepthKit.js is a plugin for visualising DepthKit volumteric captures using Three.js in WebGL. The plugin requires Three.js and a DepthKit combined-per-pixel video export from Visualise.",embed:"",excerpt:"A WebVR plugin for rendering volumetric video.",tags:["Augmented Reality","Virtual Reality","Tools"]}}}}}});
//# sourceMappingURL=path---sono-2813641fb55621dd249f.js.map